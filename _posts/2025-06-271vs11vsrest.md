---
title: "OvO and OvA"
layout: post
date: 2025-06-27
categories: tech_coding
tags:
    - machine_learning
---
当你有一个多类别分类问题（即需要将数据分到三个或更多类别中），而你使用的分类算法（比如**逻辑回归**）本身是为**二分类**设计的，这时就需要一些策略来将其扩展到多类别。One vs One (OvO) 和 One vs All (OvA) 就是两种常见的策略。

---

## One vs All (OvA) 分类器 (使用逻辑回归)

**核心思想：**
One vs All，也称为 One vs Rest (OvR)，顾名思义就是“一个类别 vs 剩下所有类别”。它将一个多类别分类问题分解成多个独立的**二分类**问题。

**工作原理：**

假设你有 $K$ 个类别（比如：猫、狗、鸟）。

1.  **训练阶段：**
    * 为每个类别训练一个独立的逻辑回归分类器。
    * 对于第 $i$ 个分类器：
        * 将第 $i$ 个类别的数据标记为**正类 (1)**。
        * 将所有其他 $K-1$ 个类别的数据合并，并标记为**负类 (0)**。
        * 使用这些标记过的数据训练一个逻辑回归模型，学习如何区分第 $i$ 个类别和所有其他类别。
    * 重复此过程 $K$ 次，最终你会得到 $K$ 个逻辑回归模型。

    例如，对于“猫、狗、鸟”的分类：
    * **分类器 1 (猫 vs 非猫)：** 训练一个模型，区分“猫”和“狗+鸟”。
    * **分类器 2 (狗 vs 非狗)：** 训练一个模型，区分“狗”和“猫+鸟”。
    * **分类器 3 (鸟 vs 非鸟)：** 训练一个模型，区分“鸟”和“猫+狗”。

2.  **预测阶段：**
    * 当有一个新的数据点需要分类时，将其输入到所有 $K$ 个训练好的逻辑回归分类器中。
    * 每个分类器都会输出一个属于其“正类”的**概率**。
    * 选择概率最高的那个分类器所对应的类别作为最终预测结果。

    例如，如果一个新图像：
    * “猫 vs 非猫”分类器预测它是猫的概率是 0.8。
    * “狗 vs 非狗”分类器预测它是狗的概率是 0.1。
    * “鸟 vs 非鸟”分类器预测它是鸟的概率是 0.05。
    * 由于“猫 vs 非猫”的概率最高 (0.8)，因此将该图像预测为“猫”。

**优点：**
* **简单易懂：** 概念直观，容易实现。
* **训练速度相对快：** 只需要训练 $K$ 个模型。
* **适用于不平衡数据集：** 如果某个类别的数据量很少，OvA 可能表现得比 OvO 好。

**缺点：**
* **类别重叠问题：** 某些区域可能会被多个分类器预测为正类，或者没有分类器预测为正类，导致决策边界不清晰。这可能需要额外的逻辑来解决冲突。
* **误差累积：** 每个分类器的错误都会传递到最终的预测中。

---

## One vs One (OvO) 分类器 (使用逻辑回归)

**核心思想：**
One vs One 的策略是为每对不同的类别训练一个二分类器。

**工作原理：**

假设你有 $K$ 个类别。

1.  **训练阶段：**
    * 需要训练 $K \times (K-1) / 2$ 个逻辑回归分类器。这个数量是组合数 $C(K, 2)$。
    * 对于每一对类别 $(i, j)$，训练一个独立的逻辑回归分类器：
        * 只使用来自类别 $i$ 和类别 $j$ 的数据进行训练。
        * 忽略所有其他类别的数据。
        * 学习如何区分类别 $i$ 和类别 $j$。

    例如，对于“猫、狗、鸟”的分类 ($K=3$)：
    * 需要训练 $3 \times (3-1) / 2 = 3$ 个分类器。
    * **分类器 1 (猫 vs 狗)：** 只用猫和狗的数据训练，区分猫和狗。
    * **分类器 2 (猫 vs 鸟)：** 只用猫和鸟的数据训练，区分猫和鸟。
    * **分类器 3 (狗 vs 鸟)：** 只用狗和鸟的数据训练，区分狗和鸟。

2.  **预测阶段：**
    * 当有一个新的数据点需要分类时，将其输入到所有 $K \times (K-1) / 2$ 个训练好的逻辑回归分类器中。
    * 每个分类器会“投票”它认为数据点属于哪个类别。
    * 统计所有投票，得票最多的类别即为最终预测结果（多数投票原则）。

    例如，一个新图像：
    * “猫 vs 狗”分类器预测为“猫”。
    * “猫 vs 鸟”分类器预测为“猫”。
    * “狗 vs 鸟”分类器预测为“狗”。
    * “猫”获得了 2 票，“狗”获得了 1 票。因此，最终预测为“猫”。

**优点：**
* **训练数据集更小：** 每个分类器只在两个类别的数据上训练，这对于每个模型来说训练时间可能会更短。
* **避免不平衡问题：** 每个二分类器面对的数据集都是相对平衡的（只包含两个类别），这有助于避免处理多类别不平衡时可能出现的问题。
* **决策边界更精细：** 由于每个分类器专注于区分两个类别，理论上可以学习到更精确的局部决策边界。

**缺点：**
* **模型数量多：** 随着类别数量 $K$ 的增加，需要训练的模型数量呈二次方增长，这会大大增加计算成本和存储需求。
* **预测时间长：** 预测时需要运行所有分类器，可能会比较慢。
* **投票冲突：** 在某些情况下，可能会出现平票，需要额外的规则来解决。

---

**总结比较：**

| 特征           | One vs All (OvA)                               | One vs One (OvO)                                    |
| :------------- | :--------------------------------------------- | :-------------------------------------------------- |
| **训练模型数** | $K$ 个                                         | $K \times (K-1) / 2$ 个                             |
| **训练数据** | 每个分类器使用所有数据，但目标变量重新标记     | 每个分类器只使用对应两个类别的数据                |
| **预测方式** | 选择概率最高的类别                             | 多数投票                                            |
| **计算成本** | 训练 $K$ 个模型，每个模型在完整数据集上训练    | 训练更多模型，但每个模型在更小数据集上训练        |
| **适用场景** | 类别数量较少时，或希望模型简单直观时            | 类别数量中等，或每个二分类问题相对容易解决时       |

在实践中，**One vs All** 往往是多类别分类的**默认选择**，因为它更简单且通常表现良好。然而，对于某些算法（例如**支持向量机 (SVM)**，其原生实现通常是二分类的），**One vs One** 策略可能会因为其在训练每个模型时处理的数据集规模较小而具有优势。对于逻辑回归，通常使用 **Softmax 回归**（也称为多项逻辑回归），它是一种直接处理多类别分类的单一模型，比 OvA 和 OvO 策略更有效。不过，OvA 和 OvO 仍然是理解如何将二分类器推广到多分类的重要概念。


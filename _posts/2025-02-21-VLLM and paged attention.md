---
title: "vLLM为什么推理如此迅速?"
layout: post
date: 2025-02-21
categories: tech_coding
tags:
    - LLM
---

最近一直在学习自然语言处理和大语言模型的一些基本概念, 这些内容是在ChatGPT, Deepseek等大语言模型的帮助下记录的。


我们先回顾一下大模型推理的基本流程：  
1. **Prefill 阶段**：模型接收到一个 prompt 时，会将整个输入序列（即前缀）经过计算，生成对应的 KV 缓存。这个阶段计算密集，但只执行一次。  
2. **Decode 阶段**：随后模型基于已有的缓存逐步生成新 token，这时主要的瓶颈转移到内存访问上，因为每生成一个 token，都要访问和更新已有的 KV 缓存。

---

### 问题：KV 缓存的内存瓶颈

由于 Transformer 模型的自注意力机制需要保存每个 token 的 Key 和 Value 向量，随着序列长度增加，KV 缓存会迅速膨胀。传统方法通常为每个请求预先分配一个固定大小的连续内存区域，这种做法会带来两方面问题：
- **内存浪费与碎片化**：很多请求实际使用的长度远小于最大值，预留大量空间造成浪费；同时，连续分配要求也使得内存利用率不高。  
- **并发处理受限**：GPU 内存资源有限，如果每个请求都“死板”地占用一块大空间，就会降低并发推理的能力。

---

### vLLM 的解决方案：PagedAttention 机制

vLLM 的核心创新正是利用了 **PagedAttention**。这一机制借鉴了操作系统中虚拟内存分页管理的思想，将 KV 缓存划分为多个**固定大小的块**，具体包括：

1. **逻辑块与物理块分离**  
   - **逻辑块**：代表请求中 token 的抽象分块，每个逻辑块存储固定数量的 token 的信息。  
   - **物理块**：是真正分配在 GPU 显存上的存储单元。逻辑块可以映射到物理块上，且映射关系由专门的 KV Cache 管理器维护。  
   
   这种分离允许物理内存在底层是非连续的，但对上层来说逻辑上是连续的，大大提高了显存的利用率，并减少了内存碎片化问题。  

2. **动态内存分配与共享前缀**  
   - 对于多个请求中相同的前缀部分，vLLM 只需计算一次 KV 缓存，然后通过逻辑到物理的映射将这部分缓存共享给其他请求，从而避免重复计算。  
   - 同时，根据当前 GPU 显存的使用情况，vLLM 会动态分配物理块。如果发现当前内存不足，系统还会采用抢占（preemption）策略，将后到的请求暂时交换到 CPU 内存，优先保证先到的请求能够尽快完成推理。  

3. **调度器与连续批处理**  
   - vLLM 内部有一个调度器，它维护三个队列：waiting（等待处理）、running（当前执行中）和 swapped（因内存不足被抢占的）。  
   - 调度器会依据 FCFS（先到先服务）原则，同时根据当前可用的物理块数量，动态决定哪些请求进入推理阶段（不论是 prefill 阶段还是 decode 阶段）。  
   - 如果 GPU 显存不够，调度器会先将部分请求的 KV 缓存交换到 CPU 上，待资源充足时再恢复执行，这样保证了系统整体的吞吐和响应时间的平衡。

---

### 总结

通过以上机制，vLLM 达到以下效果：  
- **显存利用率提升**：通过非连续物理内存和动态分配，大大降低了内存碎片，允许更多请求同时运行。  
- **计算效率提高**：共享前缀缓存减少了重复计算，预填充和解码阶段采用不同的调度策略，优化了整体推理流程。  
- **灵活的资源调度**：当遇到内存紧张时，通过抢占和交换机制，确保先到请求能够优先完成推理，兼顾了延迟和吞吐。

这种设计使得 vLLM 在大模型推理场景下能达到比传统连续内存分配方式更高的吞吐量和更低的延迟，从而成为一款高效、灵活的大模型推理引擎。



### 共享前缀指的是什么?
在大模型推理中，“共享前缀”指的是多个请求中重复出现的、位于输入开头的那部分文本序列。举例来说：

- **聊天机器人或翻译任务中**，很多请求会带有相同的系统提示、指令或示例，这些都构成了相同的前缀。
- 当这些相同的前缀出现时，模型在计算时可以只针对这部分内容生成一次 KV 缓存，然后在多个请求中复用这部分缓存，而不用重复计算。这样既节省了计算资源，也加快了推理速度。

这种设计对于提高显存利用率和整体推理效率非常有帮助，因为它避免了重复计算相同的内容，从而在并发推理场景下显著降低延迟和资源消耗。

---
title: "Reinforcement Learning Notes Week 1"
layout: post
date: 2025-05-13
categories: tech_coding
tags:
    - reinforcement_learning
---



## What is RL?
- 与其他机器学习方法的区别:
  - 监督学习 (Supervised Learning): 使用带有正确答案的标记示例进行学习。
  - 强化学习 (Reinforcement Learning): 通过奖励 (reward) 评估行为的好坏。
  - 无监督学习 (Unsupervised Learning): 提取数据中的潜在结构，用于数据表示。
- RL 的关键特征:
    - 在线学习 (online learning): 适应不断变化的环境。
  - 处理序列决策问题 (sequential decision making)。 
  - 重视智能体与环境的互动。

## Instructors
- Martha White. 
  - an Assistant Professor at the University of Alberta. 
- Adam White. 
  - a Research Professor at the University of Alberta and a Senior Researcher at DeepMind. 
  - 如何走上强化学习的研究之路: 选了一门老师看上去很和善的课, 那个老师碰巧是Rich Sutton.(我能说什么呢......羡慕羡慕超羡慕)


# An introduction to Sequencial Decision Making
## Lesson 1: The K-Armed Bandit Problem
### Define reward


**Reward (奖励)** 是指在采取一个动作后接收到的结果。在医疗试验的例子中，医生（agent）选择了一种治疗方案（action），而病人治疗后的健康状况（或者更具体地说，是血压的变化）就是医生收到的 **reward**。因此，**reward** 可以理解为对所采取行动的反馈。



### Understand the temporal nature of the bandit problem
**Temporal nature of k-bandit problems (k-臂赌博机问题的时间性)** 在这段话中并没有被直接强调。虽然文本提到了 `timesteps`（时间步）是强化学习的基本概念之一，但是对于 k-臂赌博机问题本身，这段话更侧重于在每个时间点独立地做出决策并获得即时奖励。

在医疗试验的例子中，每次病人来访，医生都需要选择一种治疗方案，这是一个独立的决策。医生会观察到这次治疗带来的 **reward**，但这个 **reward** 并没有直接影响到下一次病人来访时可用的治疗方案或者下一次决策的直接依赖关系。



### Define k-armed bandit
k-armed bandit 问题指的是一个决策者（agent）需要在 k 个不同的动作中进行选择，并根据选择的动作获得奖励。在医疗试验的例子中，医生（agent）需要从三种不同的治疗方案（k=3）中选择，每种治疗方案（action）都会产生一个未知的奖励（患者的健康状况）。

### Define action-values
action value（动作价值）是指选择某个动作所能获得的期望奖励。用概率术语来说，它是指在选择某个特定动作后所能获得的奖励的期望值。在医疗试验的例子中，action value 就是指每种治疗方案（动作）所能带来的患者健康状况改善（奖励）的平均值。用公式表示为：

$$q_*(a) \doteq \mathbb{E}[R_t | A_t = a] \quad \forall a \in \{1, ..., k\} \\
= \sum_r p(r|a) r$$

RL的目标就是最大化奖励函数.(The goal is to maximize the expected reward.)
$$\underset{a}{\operatorname{argmax}}\ q_*(a)$$
## Lesson 2: What to Learn? Estimating Action Values

### Define action-value estimation methods
Action-value estimation methods 是用来近似估计采取某个特定 action (动作) 的真实价值的技术。文本介绍了 sample-average method (样本平均方法)，其中一个 action 的估计价值通过将该 action 获得的总 reward (奖励) 除以它被选择的次数来计算。文本还描述了 incremental update rule (增量更新规则) 作为另一种以递归方式估计 action value 的方法，而无需存储所有先前的数据。
### Define exploration and exploitation
Exploitation (利用) 指的是选择当前具有最高估计价值的 action (即 greedy action (贪婪动作))，以最大化当前的 reward (奖励)。Exploration (探索) 涉及选择非贪婪的 action，以收集关于其他 action 的潜在 reward 的更多信息，即使这意味着牺牲当前的 reward。


### Select actions greedily using an action-value function
要 greedily (贪婪地) 选择 action，需要识别根据当前的 action-value function (动作价值函数) 具有最大估计价值的 action。这通常通过取估计价值的 argmax 来完成。然后，agent (代理) 选择这个 action，相信它会产生最高的 reward。

### Define online learning
Incremental update rule (增量更新规则) 允许在接收到新的数据 (reward) 时更新 action value 的估计，而无需存储和重新处理所有先前的数据。这种随着新信息不断学习和更新估计的能力是 online learning 的一个标志。
### Understand a simple online sample-average action-value estimation method
简单的 online sample-average action-value estimation method 涉及使用 incremental update rule (增量更新规则)。这个规则允许我们根据先前的估计、采取该 action 后收到的最新 reward 以及该 action 被采取的次数来计算 action 价值的新估计。这避免了存储所有过去的 reward 并在每次都从头开始重新计算平均值。文本中给出的公式是：NewEstimate = OldEstimate + (1/n) * (Reward - OldEstimate)，其中 n 是该 action 被采取的次数。
### Define the general online update equation
通用 online update equation (在线更新方程) 表示为：NewEstimate = OldEstimate + StepSize * (Target - OldEstimate)。这里，OldEstimate 是 action value 的当前估计，Target 是接收到的新信息或 reward，而 StepSize 决定了估计值向 Target 调整的程度。在 sample-average method 的情况下，StepSize 是 1/n。
### Understand why we might use a constant step-size in the case of non-stationarity
在 non-stationarity (非平稳性) 问题中，潜在的 reward 分布会随着时间而变化。使用一个恒定的 step size (介于 0 和 1 之间的值) 可以让学习 agent 更多地关注最近的 reward，而较少关注较旧的 reward。这使得 agent 对环境的变化更加敏感，并且能够更好地适应不断变化的 reward 结构，因为较旧的数据可能不再与当前情况相关。
$$\begin{align*}
Q_{n+1} &= Q_n + \alpha_n (R_n - Q_n) \\
&= \alpha_n R_n + Q_n - \alpha_n Q_n \\
&= \alpha_n R_n + (1 - \alpha_n) Q_n \\
&= \alpha_n R_n + (1 - \alpha_n) [\alpha_{n-1} R_{n-1} + (1 - \alpha_{n-1}) Q_{n-1}] \\
&= \alpha_n R_n + (1 - \alpha_n) \alpha_{n-1} R_{n-1} + (1 - \alpha_n)^2 Q_{n-1}
\end{align*}$$

## Lesson 3: Exploration vs. Exploitation Tradeoff 
### what is trade-off between exploration and exploitation
根据这段话，我们可以解释如下：

**Trade-off between exploration and exploitation (探索与利用的权衡)** 是指在决策过程中，需要在尝试新的、可能带来更好结果的选项（**exploration**，探索）和选择已知能带来好结果的选项（**exploitation**，利用）之间进行权衡。

* **Exploration (探索)** 的目的是为了改进 agent 对每个动作的了解，希望能够带来长期的利益。通过提高对动作价值的估计准确性，agent 在未来可以做出更明智的决策。例如，在餐馆的例子中，尝试新的菜品就是一种探索行为，目的是找到可能比常点的菜更好吃的选择。

* **Exploitation (利用)** 则是利用 agent 当前已知的最佳动作，选择能够带来最大 **reward** 的动作。然而，仅仅依赖于当前的估计值进行 **exploitation**，可能会导致 agent 错过实际上更好的动作。例如，如果只是一味地点以前觉得好吃的菜，就可能永远不会发现其他更好吃的菜。

这段话用一个餐馆点餐的例子很好地说明了这个权衡：你是选择点你一直觉得不错的菜（**exploitation**），还是尝试看起来很诱人的新菜（**exploration**）？两者不能同时进行。**Exploration** 可以帮助你发现更好的选择，但可能会牺牲当前的 **reward**；**exploitation** 可以让你获得当前的 **reward**，但可能会错过更好的长期回报。

### Define epsilon-greedy
**Epsilon Greedy (ε-贪婪算法)** 是一种简单的方法来平衡 **exploration** 和 **exploitation**。它的核心思想是：以一个小的概率 **epsilon**（ε）随机选择一个动作进行 **exploration**，而以较大的概率（1 - ε）选择当前估计价值最高的动作进行 **exploitation**。

用掷骰子的例子来说明，假设 ε 等于 1/6。那么每次做决策时，掷一次骰子：

* 如果骰子点数为 1（概率为 ε），则进行 **exploration**，即随机选择一个动作。
* 如果骰子点数不是 1（概率为 1 - ε），则进行 **exploitation**，即选择当前认为最好的动作（`greedy action`）。

这段话中用公式表达了 **Epsilon-Greedy** 的动作选择方式：在时间步 t，选择的动作可以是概率为 (1 - ε) 的 `greedy action`，也可以是概率为 ε 的随机动作。

**Epsilon-Greedy** 是一种简单有效的平衡 **exploration** 和 **exploitation** 的方法，通过引入一定的随机性进行 **exploration**，有助于发现潜在的最优动作，同时大部分时间仍然选择当前认为最好的动作来获取 **reward**。
![alt text](image/week1/{081ECDE6-6262-4E5A-BD1D-5E17FB1A4FA6}.png)
### Compare the short-term benefits of exploitation and the long-term benefits of exploration
exploitation (利用) 的短期收益是指选择当前已知最好的动作可以立即获得较高的奖励。而 exploration (探索) 的长期收益是指通过尝试不同的动作，agent 可能会发现一个比当前已知最好的动作更好的选择，从而在未来获得更高的累积奖励。
### Understand optimistic initial values
Optimistic initial values (乐观初始值) 是一种在强化学习中平衡 exploration 和 exploitation 的策略。它的核心思想是在学习的开始阶段，将所有可能动作的价值估计初始化为一个非常高的值，甚至高于实际可能获得的最高奖励。

在医疗试验的例子中，医生最初乐观地假设每种治疗方法都是 100% 有效的。在用显式数值表示时，所有治疗方案的初始价值都被设定为 2，这是一个乐观的估计，因为实际的奖励（病人感觉好转为 1，没有好转为 0）很可能低于这个初始值。
### Describe the benefits of optimistic initial values for early exploration
Optimistic initial values 的主要好处是能够鼓励 early exploration (早期探索)。因为所有动作的初始价值都被设置得很高，agent 在开始时会倾向于选择它认为价值最高的动作（在初始阶段，所有动作的价值都一样，所以会随机选择）。当 agent 采取一个动作并获得实际的 reward 时，如果这个 reward 低于乐观的初始值，那么该动作的价值估计就会下降。这使得其他尚未尝试过的动作看起来更有吸引力，因为它们的价值仍然保持着较高的乐观初始值。

在医疗试验的例子中，由于所有治疗的初始价值都是 2，医生会随机尝试不同的治疗方法。当病人反馈治疗效果时，即使是积极的反馈（奖励为 1），也会导致该治疗方案的价值从 2 降到 1.5（假设学习率 alpha 为 0.5）。这使得其他尚未尝试的治疗方案（价值仍然为 2）看起来更有吸引力，从而促使医生尝试所有三种治疗方案。在 10-臂赌博机的例子中，初始价值设置为 5，而实际奖励通常低于 3，这也会强制 agent 尝试不同的臂。
### Explain the criticisms of optimistic initial values
- Limited to early exploration (仅限于早期探索): 乐观初始值主要在学习的早期驱动探索。随着 agent 与环境的交互增多，价值估计会逐渐接近真实值，初始乐观值的影响会逐渐消失。这意味着 agent 在一段时间后可能停止探索，即使存在更好的动作也没有被发现。

- Issues in non-stationary problems (在非平稳问题中的问题): 在奖励分布或最优动作随时间变化的环境（non-stationary problems）中，乐观初始值可能导致问题。agent 可能已经基于早期的探索收敛到一个次优的动作，而当环境发生变化，另一个动作变得更优时，agent 可能不会重新探索并发现这个新的最优动作。

- Difficulty in setting the initial values (设置初始值的困难): 在实践中，我们可能不知道实际可能获得的最大奖励是多少，因此很难确定一个合适的乐观初始值。如果设置得太低，可能无法有效鼓励探索；如果设置得过高，可能会导致不必要的探索，或者在早期学习阶段表现不佳。
### Describe the upper confidence bound action selection method
好的，我们来解释一下这段话中提到的概念：


**Upper Confidence Bound (UCB) action selection method (置信上限动作选择方法)** 是一种用于平衡 **exploration (探索)** 和 **exploitation (利用)** 的方法。该方法的核心思想是利用对动作价值估计的不确定性来指导动作的选择。由于我们是通过采样奖励来估计动作价值的，因此我们的估计本身就存在不确定性。UCB 方法选择具有最高**upper-confidence bound (置信上限)**的动作。

这个置信上限表示我们对动作真实价值可能范围的乐观估计。对于每个动作 A，我们有一个当前的价值估计 `Q(a)`，并且围绕真实价值 `Q*(a)` 存在一个**confidence interval (置信区间)**。置信区间的上限就是**upper bound (上限)**，下限是**lower bound (下限)**。如果置信区间很小，我们对动作 A 的价值接近我们的估计值非常有信心；如果区间很大，则不确定性很高。

UCB 方法遵循 **optimism in the face of uncertainty (不确定性下的乐观主义)** 原则，即当我们对某个事物不确定时，应该乐观地假设它是好的。因此，UCB 会选择具有最高置信上限的动作。这样做的好处是，要么这个动作实际上具有很高的价值，我们能获得好的奖励；要么通过选择这个动作，我们可以更多地了解我们最不了解的动作，从而减少其价值估计的不确定性。

UCB 的动作选择公式通常是选择使以下值最大的动作：动作的估计价值加上其置信上限探索项。探索项通常包含一个用户指定的参数 C 来控制探索的程度，并且与该动作被选择的次数成反比。这意味着被选择次数较少的动作，其不确定性较高，置信上限也较高，从而更有可能被选中进行探索。
好的，UCB的公式可以用LaTeX写成如下形式：

$$a_t = \arg\max_a \left[ Q(a) + c \sqrt{\frac{\ln(t)}{N(a)}} \right]$$

其中：

* $a_t$ 是在时间步 $t$ 选择的动作。
* $Q(a)$ 是动作 $a$ 的当前估计价值。
* $c$ 是一个探索参数，用于控制探索的程度。
* $t$ 是当前的时间步。
* $N(a)$ 是到目前为止动作 $a$ 被选择的次数。

这个公式表示在每个时间步 $t$，agent 会选择那个使估计价值 $Q(a)$ 加上一个与探索相关的置信上限项最大的动作。置信上限项 $\sqrt{\frac{\ln(t)}{N(a)}}$ 随着总时间步 $t$ 的增加而增大（鼓励探索），但随着动作 $a$ 被选择次数 $N(a)$ 的增加而减小（减少对已充分探索的动作的探索）。参数 $c$ 用于调整探索和利用之间的平衡。
### Define optimism in the face of uncertainty

**Optimism in the face of uncertainty (不确定性下的乐观主义)** 是 UCB 方法背后的一个核心原则。它指的是，当我们对某个动作的真实价值存在不确定性时（即我们对其价值的估计可能不够准确，置信区间较大），我们应该乐观地假设这个动作的价值可能很高，甚至接近其估计的上限。

在 UCB 的上下文中，这意味着在选择动作时，我们不是仅仅选择当前估计价值最高的动作（这是纯粹的 **exploitation**），而是会考虑每个动作价值估计的不确定性。对于那些我们了解较少的动作（即被选择次数较少，不确定性较高），它们的置信上限会相对较高。**Optimism in the face of uncertainty** 指导我们选择具有最高置信上限的动作，因为我们乐观地认为这个动作的真实价值可能位于其估计范围的上限，从而鼓励我们去探索那些不确定性较高的动作，以便更好地了解它们，并有可能发现更优的动作。


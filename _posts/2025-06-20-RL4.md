---
title: "Reinforcement Learning Notes Week 4: Policy Evaluation and Iteration"
layout: post
date: 2025-06-20
categories: tech_coding
tags:
    - reinforcement_learning
---


## Lesson 1: Policy Evaluation (Prediction)
 

### Understand the distinction between policy evaluation and control


#### 策略评估 (Policy Evaluation)

* **定义 (Definition)**: 策略评估是指确定特定策略 ($\pi$) 的**价值函数** (**value function**) ($v_\pi$) 的过程。
* **目的 (Purpose)**: 它通过计算遵循给定策略时每个状态的预期回报来评估该策略“有多好”。
* **方法 (Method)**: **贝尔曼方程** (**Bellman equation**) 可以将此问题简化为线性方程组。虽然可以通过线性代数求解，但对于一般的马尔可夫决策过程 (MDPs)，迭代动态规划方法通常更适用。

#### Control


* **定义 (Definition)**: 控制是指找到一个能够产生最大可能奖励的策略，本质上是找到一个**最大化价值函数**的策略。
* **目标 (Goal)**: 强化学习的最终目标就是控制。
* **改进过程 (Process of Improvement)**:
    * 如果策略 ($\pi_2$) 在每个状态下的价值都大于或等于策略 ($\pi_1$) 的价值，并且至少在一个状态下严格大于，那么就认为 ($\pi_2$) 优于 ($\pi_1$)。
    * 控制任务旨在迭代地修改策略，以产生一个严格更好的策略。
    * 这种迭代改进会一直持续，直到找不到严格更好的策略为止，这表明已经达到了一个**最优策略** (**optimal policy**)。

---

### Explain the setting in which dynamic programming can be applied, as well as its limitations
#### Dynamic Programming


* **应用 (Application)**: 动态规划算法用于解决策略评估和控制问题。
* **前提 (Prerequisite)**: 这些算法需要访问**环境动力学** (**dynamics of the environment**) ($p$)，即环境行为的模型。
* **机制 (Mechanism)**: 动态规划利用**贝尔曼方程** (**Bellman equations**) 来定义这两个任务的迭代算法。
* **性质 (Nature)**: 经典的动态规划不涉及与环境的交互；相反，它基于 MDP 模型计算价值函数和最优策略。
* **重要性 (Importance)**: 尽管它依赖于模型，但动态规划对于理解其他强化学习算法至关重要，因为许多算法可以被视为在没有模型的情况下对动态规划的近似。

---

 


### Outline the iterative policy evaluation algorithm for estimating state values under a given policy

迭代策略评估（Iterative Policy Evaluation）算法通过将 Bellman 方程直接用作更新规则来估计给定策略 $\pi$ 下的状态值函数 $V^\pi$。其核心思想是不断迭代地细化对价值函数的估计，直到收敛。

**算法步骤概览：**

1.  **初始化 (Initialization):**
    * 为每个状态 $S$ 初始化一个近似价值函数 $V_0(S)$。通常，所有状态的初始值都设置为0。
    * 需要两个数组：一个存储当前近似价值函数 $V$，另一个存储更新后的价值函数 $V'$。

2.  **迭代更新 (Iterative Update):**
    * **Sweep (扫描):** 在每次迭代中，遍历状态空间中的所有状态 $S$。
    * **应用更新规则 (Apply Update Rule):** 对于每个状态 $S$，使用 Bellman 方程作为更新规则来计算其新的价值 $V'(S)$：
        $V'(S) = \sum_{A} \pi(A|S) \sum_{S'} P(S'|S, A) [R(S, A, S') + \gamma V(S')]$
        其中：
        * $\pi(A|S)$ 是在状态 $S$ 下采取行动 $A$ 的概率（由给定策略决定）。
        * $P(S'|S, A)$ 是在状态 $S$ 下采取行动 $A$ 后转移到状态 $S'$ 的概率。
        * $R(S, A, S')$ 是从状态 $S$ 采取行动 $A$ 转移到状态 $S'$ 所获得的奖励。
        * $\gamma$ 是折扣因子。
        * 重要的是，在计算 $V'(S)$ 时，使用的是当前 $V$ 数组中存储的旧值 $V(S')$。
    * **复制更新 (Copying Updates):** 完成一轮所有状态的扫描后，将 $V'$ 数组中的所有新值复制回 $V$ 数组，为下一次迭代做准备。

3.  **收敛判断 (Convergence Check):**
    * 引入一个参数 $\Delta$ 来跟踪在一次迭代中所有状态价值的最大变化量。
    * 算法的外部循环持续进行，直到 $\Delta$ 小于一个预先设定的阈值 $\theta$（一个用户指定的小常数，例如0.001）。
    * 当近似价值函数不再发生显著变化时（即 $\Delta < \theta$），表示算法已经收敛到真正的价值函数 $V^\pi$。这是因为 $V^\pi$ 是 Bellman 方程的唯一解。

**特殊情况：**
* 终止状态（Terminal states）的价值定义为0，因此它们的价值不会被更新。
* 虽然文本中主要描述了使用两个数组的版本，但也可以使用一个数组来实现，这种情况下通常收敛更快，因为它更早地使用了更新后的值。

### Apply iterative policy evaluation to compute value functions

迭代策略评估通过上述步骤，在具体例子中计算价值函数。

**示例：4x4 Grid World**

1.  **环境设置：**
    * 一个4x4的网格世界，顶左角和底右角是终止状态。
    * 每次转移的奖励 (Reward) 都是 -1。
    * 无折扣因子 $\gamma = 1$（因为是 Episodic MDP）。
    * 每个状态有四个可能的行动：上、下、左、右。行动是确定性的，如果行动会使智能体离开网格，则智能体留在原地。
2.  **策略 (Policy)：**
    * 均匀随机策略：每个行动被选择的概率都是 1/4。
    * 价值函数 $V^\pi$ 代表从给定状态到达终止状态的预期步数（因为奖励是 -1）。

3.  **初始化：**
    * 将所有非终止状态的初始价值 $V(S)$ 设置为 0。

4.  **第一次 Sweep (迭代)：**
    * 按照从左到右、从上到下的顺序扫描状态（顺序不重要，因为使用两个数组）。
    * **以状态1为例（假设状态编号从1开始）：**
        * 计算 $V'(1)$：
            * 对于“左”行动（概率 1/4）：转移到终止状态（价值为0），奖励为 -1。所以贡献是 $1/4 \times (-1 + 0) = -0.25$。
            * 对于其他行动（上、右、下），类似地，奖励为 -1，下一个状态的旧价值为0。
            * 所有行动的贡献相加，最终 $V'(1)$ 被设置为 -1。
    * **以状态2为例：**
        * 计算 $V'(2)$：
            * 对于“左”行动（概率 1/4）：转移到状态1，奖励为 -1。使用旧的 $V(1)=0$，所以贡献是 $1/4 \times (-1 + 0) = -0.25$。
            * 所有行动的贡献相加，最终 $V'(2)$ 也被设置为 -1。
    * 实际上，由于所有初始状态价值都为0，并且每次转移奖励都是 -1，在第一次扫描后，所有非终止状态的 $V'$ 值都将变为 -1。
    * 完成扫描后，将 $V'$ 的值复制回 $V$。
    * 最大变化 $\Delta$ 为 1.0（因为从0变到-1）。如果 $\theta = 0.001$，则继续下一轮迭代。
5.  **后续 Sweeps：**
    * 随着迭代的进行，终止状态的影响会逐渐向外传播。靠近终止状态的区域的价值会首先开始变化，反映出它们距离终止状态的预期步数。
    * 例如，在第二次 Sweep 后，靠近终止状态的那些状态的价值将开始变得更负（例如 -2 或 -3），因为它们现在考虑到了从相邻状态转移到终止状态的可能性。
    * 随着迭代次数的增加，价值函数的近似值会越来越接近真实的 $V^\pi$。
6.  **收敛：**
    * 算法持续运行，直到在一次完整的扫描中，所有状态价值的最大变化量 $\Delta$ 小于预设的 $\theta$ 值。
    * 当达到这个条件时，算法停止，此时的 $V$ 数组中存储的值就是给定随机策略下的收敛的价值函数 $V^\pi$。这些值代表了从每个状态到终止状态的预期步数。
通过这个过程，迭代策略评估能够有效地计算给定策略下的状态价值函数。
---
公式总结:

Iterative Policy Evaluation, for estimating $V \approx V_{\pi}$

**Input:** $\pi$, the policy to be evaluated

**Initialization:**
$\vec{V} \leftarrow \vec{0}$, $\vec{V'} \leftarrow \vec{0}$

**Loop:**
&nbsp;&nbsp;&nbsp;&nbsp;$\Delta \leftarrow 0$
&nbsp;&nbsp;&nbsp;&nbsp;Loop for each $s \in \mathcal{S}$:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V'(s) \leftarrow \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) [r + \gamma V(s')]$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\Delta \leftarrow \max(\Delta, |V'(s) - V(s)|)$
&nbsp;&nbsp;&nbsp;&nbsp;$V \leftarrow V'$
**until** $\Delta < \theta$ (a small positive number)

**Output:** $V \approx V_{\pi}$

---


## Lesson 2: Policy Iteration (Control) 
 
### Understand the policy improvement theorem

策略改进定理（Policy Improvement Theorem）是强化学习中的一个重要理论，它解释了如何从一个给定策略的价值函数中推导出更好的策略。

**核心思想：**
* 如果我们有一个策略 $\pi$ 的价值函数 $V^\pi$，我们可以通过贪婪地选择行动来创建一个新的策略 $\pi'$。这个新策略 $\pi'$ 在每个状态下选择能够最大化 $Q^\pi(s, a)$ 的行动，其中 $Q^\pi(s, a)$ 是在状态 $s$ 采取行动 $a$ 后，接着遵循策略 $\pi$ 所能获得的预期回报。
* **定理表述：** 对于任意两个策略 $\pi$ 和 $\pi'$，如果对于所有状态 $s \in \mathcal{S}$，以下条件成立：
    $Q^\pi(s, \pi'(s)) \ge V^\pi(s)$
    那么策略 $\pi'$ 至少与策略 $\pi$ 一样好，即 $V^{\pi'}(s) \ge V^\pi(s)$ 对于所有 $s \in \mathcal{S}$。
    如果至少有一个状态 $s$ 满足 $Q^\pi(s, \pi'(s)) > V^\pi(s)$，则策略 $\pi'$ 是严格优于策略 $\pi$ 的，即 $V^{\pi'}(s) > V^\pi(s)$。

**意义：**
* 这个定理保证了，如果我们对当前策略 $\pi$ 的价值函数 $V^\pi$ 采取贪婪（Greedy）行动来构建一个新的策略 $\pi'$，那么除非 $\pi$ 已经是最优策略，否则 $\pi'$ 一定会是一个严格改进的策略。
* 如果贪婪化操作没有改变策略 $\pi$，则说明 $\pi$ 已经相对于它自己的价值函数是贪婪的，这意味着 $\pi$ 已经满足 Bellman Optimality Equation，因此 $\pi$ 已经是最优策略。

### Use a value function for a policy to produce a better policy for a given MDP

要使用一个策略的价值函数来生成一个更好的策略，可以遵循以下步骤：

1.  **获得当前策略的价值函数 $V^\pi$：**
    * 首先，需要通过像“迭代策略评估”（Iterative Policy Evaluation）这样的方法，计算出当前给定策略 $\pi$ 的状态价值函数 $V^\pi$。这意味着对于MDP中的每个状态 $s$，我们都知道在遵循策略 $\pi$ 的情况下，从该状态开始所能获得的预期总回报。

2.  **构建 $Q^\pi$ 函数：**
    * 基于 $V^\pi$，我们可以计算出行动价值函数 $Q^\pi(s, a)$。 $Q^\pi(s, a)$ 表示在状态 $s$ 采取行动 $a$，然后继续遵循策略 $\pi$ 所能获得的预期回报。其计算公式为：
        $Q^\pi(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma V^\pi(s')]$
        其中：
        * $p(s', r | s, a)$ 是在状态 $s$ 采取行动 $a$ 后转移到状态 $s'$ 并获得奖励 $r$ 的概率。
        * $\gamma$ 是折扣因子。

3.  **贪婪化（Greedification）以生成新策略 $\pi'$：**
    * 对于每个状态 $s$，选择能最大化 $Q^\pi(s, a)$ 的行动 $a$ 作为新策略 $\pi'(s)$ 的行动。
    * 形式上，新策略 $\pi'$ 定义为：
        $\pi'(s) = \arg\max_a Q^\pi(s, a)$
    * 如果有多个行动产生相同的最大 $Q^\pi$ 值，可以选择其中任意一个，或者以任意方式在它们之间分配概率。

**示例（4x4 Grid World）：**

* **输入：** 假设我们已经通过迭代策略评估获得了均匀随机策略的最终价值函数 $V^\pi$（如视频中所示）。这些价值通常是负数，因为奖励是 -1。
* **应用贪婪化：** 对于每个状态 $s$，观察其相邻状态 $s'$ 的 $V^\pi(s')$ 值。选择能导致 $s'$ 具有“最大”（即“最不负”）$V^\pi(s')$ 值的行动。
* **生成新策略 $\pi'$：**
    * 例如，在一个状态中，向上移动可能导致一个价值为 -5 的状态，向下移动可能导致一个价值为 -4 的状态。那么，新策略 $\pi'$ 会选择向下移动，因为它导致了更高的（更不负的）预期回报。
    * 这样，对于所有非终止状态，我们都会选择一个行动，使得它能引导智能体走向距离终止状态更近的路径（因为近的路径意味着更少的负奖励，即更高的价值）。
* **结果：** 视频中显示，即使从一个非最优的 $V^\pi$ 开始，通过这种贪婪化过程，可以生成一个比原始策略更好的策略 $\pi'$。在某些情况下，就像这个Grid World例子一样，新策略甚至可能直接就是最优策略，因为它引导智能体沿着最短路径走向终止状态。

 


### Outline the policy iteration algorithm for finding the optimal policy

策略迭代（Policy Iteration）算法是一种用于寻找最优策略（Optimal Policy）的动态规划方法。它通过交替执行“策略评估”（Policy Evaluation）和“策略改进”（Policy Improvement）这两个核心步骤，来生成一系列越来越好的策略，直到找到最优策略为止。

**算法步骤概览：**

1.  **初始化 (Initialization):**
    * 任意初始化一个策略 $\pi_0$（例如，均匀随机策略）。
    * 任意初始化一个状态价值函数 $V(s)$（例如，所有状态的价值设为0）。

2.  **策略评估 (Policy Evaluation) 步骤：**
    * 使用“迭代策略评估”（Iterative Policy Evaluation）算法来计算当前策略 $\pi_k$ 的精确价值函数 $V^{\pi_k}$。
    * 这个过程会反复更新 $V(s)$，直到其收敛到 $V^{\pi_k}$，即 $V(s)$ 的变化量小于一个预设的阈值 $\theta$。

3.  **策略改进 (Policy Improvement) 步骤：**
    * 基于刚刚计算出的 $V^{\pi_k}$，生成一个新的、改进的策略 $\pi_{k+1}$。
    * 对于每个状态 $s$，新策略 $\pi_{k+1}(s)$ 选择能够最大化 $Q^{\pi_k}(s, a)$ 的行动 $a$：
        $\pi_{k+1}(s) = \arg\max_a \sum_{s', r} p(s', r | s, a) [r + \gamma V^{\pi_k}(s')]$
    * 检查新策略 $\pi_{k+1}$ 是否与旧策略 $\pi_k$ 相同。如果所有状态的行动选择都保持不变，则称策略是“稳定的”（policy-stable）。

4.  **终止条件 (Termination Condition):**
    * 如果在策略改进步骤中，新的策略 $\pi_{k+1}$ 与旧的策略 $\pi_k$ 完全相同（即 policy-stable 为 true），则表示算法已经收敛到最优策略。此时，当前的策略 $\pi_k$ 就是最优策略 $\pi_*$，当前的价值函数 $V^{\pi_k}$ 就是最优价值函数 $V_*$。算法终止。
    * 否则（如果策略发生了变化），返回到步骤2，用新的策略 $\pi_{k+1}$ 重新进行策略评估。

**收敛保证：**
* 策略改进定理保证了每次改进都会生成一个严格更好的策略（除非已经达到最优）。
* 由于确定性策略的数量是有限的，这个迭代过程最终一定会收敛到最优策略和最优价值函数。

---
**Policy Iteration (using iterative policy evaluation) for estimating $\pi \approx \pi_*$**

**1. Initialization**
$V(s) \in \mathbb{R}$ and $\pi(s) \in A(s)$ arbitrarily for all $s \in S$

**2. Policy Evaluation**
Loop:
&nbsp;&nbsp;&nbsp;&nbsp;$\Delta \leftarrow 0$
&nbsp;&nbsp;&nbsp;&nbsp;Loop for each $s \in S$:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v \leftarrow V(s)$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(s) \leftarrow \sum_{s',r} p(s',r|s,\pi(s))[r + \gamma V(s')]$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\Delta \leftarrow \max(\Delta, |v - V(s)|)$
&nbsp;&nbsp;&nbsp;&nbsp;until $\Delta < \theta$ (a small positive number determining the accuracy of estimation)

**3. Policy Improvement**
&nbsp;&nbsp;&nbsp;&nbsp;policy-stable $\leftarrow$ true
&nbsp;&nbsp;&nbsp;&nbsp;For each $s \in S$:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;old-action $\leftarrow \pi(s)$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\pi(s) \leftarrow \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If old-action $\neq \pi(s)$, then policy-stable $\leftarrow$ false
&nbsp;&nbsp;&nbsp;&nbsp;If policy-stable, then stop and return $V \approx V_*$ and $\pi \approx \pi_*$; else go to 2

---

### Understand “the dance of policy and value”

“策略与价值之舞”（The dance of policy and value）形象地描述了策略迭代算法中策略评估和策略改进这两个步骤之间交替进行的关系，以及它们如何共同趋向最优解。
1.  **初始状态：** 我们从一个任意的策略 $\pi_0$ 和一个初始价值函数 $V_0$ 开始。此时，$V_0$ 通常与 $\pi_0$ 不匹配，也可能不准确。

2.  **策略评估 (Evaluation Step)：**
    * 在这一步中，我们固定策略 $\pi_k$，并使用迭代策略评估来计算其真实的价值函数 $V^{\pi_k}$。
    * 此时，价值函数 $V^{\pi_k}$ 变得“准确”（accurate），因为它精确反映了当前策略 $\pi_k$ 的预期回报。然而，由于我们没有改变策略，这个 $V^{\pi_k}$ 对于我们接下来要找的更好策略而言，可能并不是“贪婪的”（greedy）。

3.  **策略改进 (Improvement Step)：**
    * 接着，我们利用刚刚得到的准确价值函数 $V^{\pi_k}$，通过贪婪化（Greedification）操作来生成一个新的、改进的策略 $\pi_{k+1}$。
    * 此时，新策略 $\pi_{k+1}$ 对于 $V^{\pi_k}$ 而言是“贪婪的”，因为它在每个状态下都选择了能够最大化基于 $V^{\pi_k}$ 的 $Q$ 值的行动。但是，这个 $V^{\pi_k}$ 不再准确地反映新策略 $\pi_{k+1}$ 的真实价值了（因为策略已经改变）。

4.  **循环往复：**
    * 这种“策略准确但价值不贪婪”和“价值贪婪但策略不准确”的循环交替进行。
    * 每次迭代都会使得策略和价值函数朝着最优解的方向前进。
    * 这种“跳舞”可以被可视化为在两个“线”之间来回跳跃：一条是“价值函数准确反映策略”的线（$V=V^\pi$），另一条是“策略相对于价值函数是贪婪的”的线（$\pi = \text{greedy}(V)$）。这两条线只在一个点相交，即最优策略 $\pi_*$ 和最优价值函数 $V_*$。

5.  **收敛：** 当策略改进步骤无法再改变策略时，意味着当前的策略已经相对于自己的价值函数是贪婪的，并且价值函数也准确反映了该策略的价值。此时，算法收敛，并且我们找到了最优策略和最优价值函数。

### Apply policy iteration to compute optimal policies and optimal value functions

策略迭代通过不断地交替进行策略评估和策略改进，最终计算出最优策略和最优价值函数。

**应用过程示例（改进后的4x4 Grid World）：**

1.  **问题设定：**
    * 4x4 网格世界，只有一个终止状态。
    * 除了常规的 -1 奖励，存在“坏状态”（Bad States，蓝色格子），进入这些状态会获得 -10 的奖励。
    * 目标是找到避免蓝色格子并到达终止状态的最短/最优路径。

2.  **初始化：**
    * 初始策略 $\pi_0$：均匀随机策略（每个方向 1/4 概率）。
    * 初始价值函数 $V(s)$：所有状态价值设为 0。


3.  **第一次迭代：**
    * **策略评估：** 对 $\pi_0$（均匀随机策略）进行评估。由于存在坏状态，评估后会发现离坏状态近的格子价值变得非常负，而离终止状态近的格子价值相对不那么负。
    * **策略改进：** 基于 $V^{\pi_0}$ 贪婪化，生成 $\pi_1$。此时，靠近终止状态的区域可能会开始出现正确的路径（比如向上走），但远离终止状态的区域（特别是靠近坏状态的区域）仍然可能选择次优的路径，例如，从底部行直接穿过坏状态。策略会发生改变，因为“old-action”不再等于“$\pi(s)$”。

4.  **第二次迭代：**
    * **策略评估：** 对 $\pi_1$ 进行评估。由于 $\pi_1$ 已经有所改进，其价值函数 $V^{\pi_1}$ 会比 $V^{\pi_0}$ 更“合理”，即其值会普遍更高（或更不负），并且更准确地反映 $\pi_1$ 的行为。例如，底部右侧格子的价值会从 -15 变为 -6，因为它现在能更有效地避免坏状态。
    * **策略改进：** 基于 $V^{\pi_1}$ 贪婪化，生成 $\pi_2$。此时，更多状态（例如底部右侧）的行动选择会得到改进，开始沿着最优路径移动，进一步避开蓝色格子。

5.  **持续迭代：**
    * 策略评估和策略改进的循环会不断进行。
    * 在每次策略评估中，$V$ 会变得更准确。
    * 在每次策略改进中，$\pi$ 会变得更好。
    * 这个过程会逐渐“剪除”搜索空间，使得智能体远离高惩罚区域，并沿着最小成本路径前进。

6.  **收敛：**
    * 最终，会达到一个状态：当进行策略改进时，没有一个状态的行动选择发生变化（即 policy-stable 为 true）。
    * 此时，算法停止。当前的策略就是最优策略 $\pi_*$，当前的价值函数就是最优价值函数 $V_*$。在这个例子中，最优策略会完全避开蓝色格子，沿着白色路径到达终止状态。
这个例子展示了策略迭代的强大之处：它能够通过交替的评估和改进步骤，即使在复杂的问题中，也能保证找到最优策略和最优价值函数。
 
 


## Lesson 3: Generalized Policy Iteration
 


### Understand the framework of generalized policy iteration

广义策略迭代（Generalized Policy Iteration, GPI）是一个统一的框架，它描述了策略评估（Policy Evaluation）和策略改进（Policy Improvement）步骤之间的任何交错（interleaving）方式，同时仍能保证收敛到最优策略。

**核心思想：**
* 策略迭代（Policy Iteration）是一种特殊的 GPI，它严格地将策略评估运行到完成，然后进行策略改进，再重复。
* GPI 则放宽了这种严格性。在 GPI 中，策略评估和策略改进不必都运行到完成。
    * **策略评估：** 每次迭代可以只让当前策略的价值估计“靠近”其真实价值一点点，而不是完全收敛。
    * **策略改进：** 每次迭代可以只让策略“更趋向于贪婪”，而不是完全贪婪。
* **目标：** 尽管步骤的严格性有所放松，GPI 框架下的所有方法仍然保证最终会收敛到最优策略和最优价值函数。
* **“策略与价值之舞”的扩展：** 策略迭代中的“策略与价值之舞”是 GPI 的一个特例。GPI 允许更灵活地在“价值函数准确反映策略”和“策略相对于价值函数是贪婪的”这两个目标之间进行“跳舞”，只要两者都能持续推进，最终就能达到两者的唯一交点——最优策略和最优价值函数。

### Outline value iteration, an important example of generalized policy iteration

价值迭代（Value Iteration）是广义策略迭代（GPI）的一个重要特例。它将策略评估和策略改进步骤合并成一个单一的更新规则，直接作用于状态价值函数。

**算法步骤概览：**

1.  **初始化 (Initialization):**
    * 任意初始化状态价值函数 $V(s)$（通常所有状态的价值设为0）。

2.  **迭代更新 (Iterative Update):**
    * **单次 Sweep (扫描):** 在每次迭代中，遍历状态空间中的所有状态 $S$。
    * **应用更新规则 (Apply Update Rule):** 对于每个状态 $S$，使用一个特殊的更新规则来计算其新的价值 $V(S)$。这个更新规则结合了 Bellman Optimality Equation 的思想，因为它选择能够最大化预期回报的行动：
        $V(S) \leftarrow \max_A \sum_{S', R} P(S', R | S, A) [R + \gamma V(S')]$
        这里，$P(S', R | S, A)$ 是在状态 $S$ 采取行动 $A$ 后转移到状态 $S'$ 并获得奖励 $R$ 的概率，$\gamma$ 是折扣因子。
        **关键点：** 这个更新规则没有引用任何显式的策略 $\pi$，它直接根据当前价值估计的最大化行动来更新价值。

3.  **收敛判断 (Termination Condition):**
    * 与迭代策略评估类似，当在一个完整的扫描中，所有状态价值的最大变化量 $\Delta$ 小于一个预设的阈值 $\theta$（一个小的正数）时，算法终止。
    * 价值迭代保证最终会收敛到最优价值函数 $V^*$。

4.  **恢复最优策略 (Recovering Optimal Policy):**
    * 一旦价值函数 $V(s)$ 收敛到 $V^*$，可以通过对 $V^*$ 进行一次贪婪化操作来恢复最优策略 $\pi^*$：
        $\pi^*(s) = \arg\max_A \sum_{S', R} P(S', R | S, A) [R + \gamma V^*(S')]$

**特点：**
* 价值迭代可以看作是策略迭代的一个极端情况，其中在每次策略评估中只执行一次扫描，然后立即进行策略改进。
* 因为更新规则直接最大化价值，所以其名称为“价值迭代”。
---

**Value Iteration, for estimating $\pi \approx \pi_*$**

Algorithm parameter: a small threshold $\theta > 0$ determining accuracy of estimation
Initialize $V(s)$, for all $s \in S^+$, arbitrarily except that $V(terminal) = 0$

Loop:
&nbsp;&nbsp;&nbsp;&nbsp;$\Delta \leftarrow 0$
&nbsp;&nbsp;&nbsp;&nbsp;Loop for each $s \in S$:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v \leftarrow V(s)$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**<span style="color: red;">$V(s) \leftarrow \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$</span>**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\Delta \leftarrow \max(\Delta, |v - V(s)|)$
&nbsp;&nbsp;&nbsp;&nbsp;until $\Delta < \theta$

Output a deterministic policy, $\pi \approx \pi_*$, such that
$\pi(s) = \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$

---

### Understand the distinction between synchronous and asynchronous dynamic programming methods

同步（Synchronous）和异步（Asynchronous）是动态规划方法中更新状态价值的两种不同方式。

1.  **同步动态规划方法 (Synchronous Dynamic Programming Methods):**
    * **定义：** 这类方法在每次迭代中都对整个状态空间进行系统的、全面的扫描（sweep）。这意味着在一次迭代中，所有状态的价值都会被计算（使用上一次迭代的价值函数），然后同时更新。
    * **例子：** 策略迭代（Policy Iteration）和价值迭代（Value Iteration）都属于同步方法，因为它们在每次循环中都会遍历并更新所有状态。
    * **优点：** 概念清晰，实现相对简单，收敛性证明直接。
    * **缺点：** 当状态空间非常大时，每次全面的扫描可能耗时巨大，效率低下。

2.  **异步动态规划方法 (Asynchronous Dynamic Programming Methods):**
    * **定义：** 这类方法不进行系统的扫描。它们以任意顺序更新状态的价值，可以频繁更新某些状态，而其他状态可能长时间不被更新。
    * **收敛保证条件：** 为了保证收敛，异步算法必须确保所有状态的价值最终都会被持续更新（例如，不能永远只更新少数几个状态而忽略其他状态）。
    * **例子：**
        * **in-place dynamic programming：** 仅使用一个数组来存储价值函数，更新状态时立即使用已经更新过的相邻状态的新价值。
        * **prioritized sweeping：** 优先更新那些价值变化较大或对其他状态价值影响较大的状态。
        * **real-time dynamic programming：** 根据智能体的实际经验来选择更新哪些状态。
    * **优点：**
        * **效率提升：** 可以有选择性地更新状态，从而更快地传播价值信息，尤其在状态空间巨大时可能比同步方法更高效。
        * **灵活性：** 允许根据问题特点或实际情况自由选择更新顺序。
    * **缺点：** 收敛性证明通常比同步方法复杂，实现可能更复杂。

**总结：** 同步方法是“一视同仁”地更新所有状态，而异步方法则是“选择性地”更新状态。异步方法通过牺牲部分更新的系统性来换取潜在的效率提升。
 
---

好的，这里是策略迭代（Policy Iteration）和价值迭代（Value Iteration）的简略对比：

### 策略迭代 (Policy Iteration)

* **流程：** `评估策略 (Policy Evaluation) -> 改进策略 (Policy Improvement)`，重复直到策略稳定。
* **目的：** 找到最优策略 $\pi^*$ 和最优价值函数 $V^*$。
* **核心：** 严格地评估当前策略的价值，然后根据该价值贪婪地改进策略。
* **特点：**
    * 外部迭代次数少（策略改进次数）。
    * 每次内部迭代（策略评估）需要多次扫描直到收敛，开销可能较大。
    * 确保收敛到最优。

### 价值迭代 (Value Iteration)

* **流程：** `直接更新价值函数（融合评估与改进）`，重复直到价值函数稳定。
* **目的：** 找到最优价值函数 $V^*$，然后从中恢复最优策略 $\pi^*$。
* **核心：** 使用 Bellman Optimality Equation 直接更新价值，隐式地进行策略改进。
* **特点：**
    * 外部迭代次数可能较多（价值更新次数）。
    * 每次迭代只进行一次价值函数扫描，计算效率高。
    * 确保收敛到最优。

### 核心区别：

* **策略迭代：** “先算准，再改进”。
* **价值迭代：** “边算边改进，最终算准”。

两者都属于动态规划方法，且都能找到最优解，但收敛路径和每次迭代的计算粒度不同。


---

---
### 描述 Brute-Force Search 作为寻找最优策略的替代方法
`Describe brute force search as an alternative method for searching for an optimal policy `

**Brute-force search** 是一种直接但效率低下的方法，用于寻找最优策略。它通过评估每一个可能的确定性策略来工作，然后选择价值最高的策略。虽然这种方法最终会找到最优解（因为存在有限数量的确定性策略，并且总是存在一个最优的确定性策略），但它的主要缺点在于效率极低。一个确定性策略涉及每个状态的一个动作选择，这意味着确定性策略的总数随着状态数量的增加呈指数级增长。即使对于一个相对简单的问题，策略空间也可能非常庞大，导致搜索过程耗时极长。

---
### 描述 Monte Carlo 作为学习价值函数的替代方法
`Describe Monte Carlo as an alternative method for learning a value function`

**Monte Carlo method** 是一种基于采样的方法，用于估计每个状态的价值。它将每个状态的价值视为一个完全独立的估计问题。其过程是：首先，在给定策略 $\pi$ 下收集大量回报（returns），然后计算这些回报的平均值。随着收集到的回报数量的增加，这个平均值会逐渐收敛到真实的状态价值。然而，这种方法可能需要从每个状态收集大量回报，因为每个回报都依赖于策略 $\pi$ 选择的许多随机动作以及 MDP 动态导致的许多随机状态转移。这引入了大量的随机性，使得单个回报可能与真实状态价值相去甚远，因此需要对许多回报进行平均才能使估计收敛，并且这个过程必须针对每个状态独立进行。

---
### 理解 Dynamic Programming 和 “Bootstrapping” 相对于这些替代策略的优势
`Understand the advantage of Dynamic programming and “bootstrapping” over these alternative strategies for finding the optimal policy`

**Dynamic Programming** 相较于 Monte Carlo 和 brute-force search 具有显著优势，主要体现在以下几个方面：

* **Bootstrapping 的效率**：Dynamic Programming 的关键在于 **bootstrapping**。它不将每个状态的评估视为一个独立的问题，而是利用已经计算出的其他价值估计来改进当前的价值估计，即利用后继状态的价值估计来更新当前状态的价值估计。这种方法比 Monte Carlo 方法独立估计每个价值要高效得多。

* **Policy Iteration 的效率**：Dynamic Programming 中的 **Policy Iteration** 算法保证在多项式时间内（相对于状态和动作的数量）找到最优策略。这使得 Dynamic Programming 比 brute-force search（其时间复杂度呈指数级增长）在寻找最优策略方面具有指数级的速度优势。在实践中，Dynamic Programming 通常比其最坏情况下的保证还要快得多，即使是相对复杂的问题也能在较少的迭代次数内收敛。

* **迭代评估的快速收敛**：尽管 Policy Iteration 的每个步骤都需要运行 Policy Evaluation 直到完成，但在实践中这并不是一个限制。随着每次迭代，策略的变化越来越小，导致价值函数的变化也越来越小，因此 Policy Evaluation 步骤通常会很快终止。

尽管 **Curse of Dimensionality**（维度灾难）意味着随着状态变量数量的增加，状态空间的大小呈指数级增长，使得遍历所有状态变得困难，但这并非 Dynamic Programming 本身的问题，而是强化学习领域面临的普遍挑战。Dynamic Programming 通过其内在的效率和 bootstrapping 机制，有效地缓解了在处理大规模问题时可能遇到的计算负担，使其成为解决 MDPs 的强大工具。
 

 

 

----
## 一些另外的笔记
好的，我们用一个更生活化的例子来理解 **策略评估 (Policy Evaluation)** 和 **策略改进 (Policy Improvement)**。

假设你正在玩一个游戏，目标是获得最高分。这个游戏有很多不同的“**状态**”（比如你站在哪个格子、手上有哪些道具），你可以在每个状态下采取不同的“**行动**”（比如往前走、使用道具）。当你采取行动时，会得到“**奖励**”（得分或扣分），并进入一个新的状态。

---

### 策略评估 (Policy Evaluation)：“算分高手”

想象一下，你现在已经决定了一种固定的**玩法**（这就是你的“**策略**”，Policy），比如你总是习惯性地往前走，遇到敌人就用火球，遇到宝箱就打开。

**策略评估的目的，就是评估你这种固定玩法“有多好”。**

具体来说，策略评估就是：
* **为你游戏中的每一个“状态”（每一个格子、每一种道具组合），计算一个“预期分数”。**
* 这个“预期分数”是什么意思呢？就是说，如果你从这个状态开始，**并且严格按照你当前设定的玩法（策略）一直玩下去，平均下来你能获得多少总分。**

**为什么需要这个“预期分数”？**
有了每个状态的预期分数，你就能知道：“哦，如果我到了这个危机四伏的角落，按我现在的玩法，我平均会扣多少分；但如果我到了那个安全区，平均又能加多少分。”这就像你请了一个“算分高手”，他能精确计算你当前玩法下，从任何一个地方开始玩能得多少分。

**怎么算呢？**
它不是真的玩很多次，而是利用游戏规则（比如：往前走会到哪个格子，会得多少分，遇到敌人会发生什么），**通过计算和迭代（就像我们说的“贝尔曼方程”），一步一步地把每个状态的预期分数算出来。** 就像你的算分高手在纸上推演，而不是真的玩很多局。他会反复推算，直到每个状态的预期分数都稳定下来，不再变化。

**所以，策略评估就是：给定一种玩法，算出这种玩法下，从每个地方开始玩平均能得多少分。**

---

### 策略改进 (Policy Improvement)：“优化大师”

你现在已经有了算分高手帮你算出来的每个状态的预期分数了。你知道了你当前的玩法从哪个状态开始能得多少分。

**策略改进的目的，就是利用这些预期分数，来改进你的玩法，让它变得更好。**

具体来说，策略改进就是：
* **看着这些预期分数，对于游戏中的每一个“状态”，你问自己：“如果我现在在这个状态，我能不能采取一个行动，让我去到的下一个状态，能获得更高的预期分数？”**
* 比如说，你当前玩法下，在A状态你习惯向右走，预期分数是50。但你发现，如果我在A状态向左走，虽然当前奖励不多，但能把我带到B状态，而B状态根据“算分高手”的计算，它未来的预期分数是80！
* 那么，作为“优化大师”，你就会立刻调整你的玩法：“好，从现在开始，在A状态，我不再向右走，我改向左走！”

**这个过程的关键在于“贪婪地选择”：**
你总是选择那个能让你下一步进入“最好”（预期分数最高）状态的行动。如果你的当前玩法已经让你达到了每个状态下的最优选择，那说明你的玩法已经是最优的了，不需要再改进了。

**所以，策略改进就是：根据当前玩法的预期分数，调整每个状态下的行动选择，让新玩法比旧玩法更好。**

---

### 两者结合：“策略迭代”—— 从新手到大师的蜕变

**策略迭代 (Policy Iteration)** 就是不断重复上面这两个过程：

1.  **策略评估：** 你先用你当前的玩法（策略），让“算分高手”帮你把每个状态的预期分数都算清楚。
2.  **策略改进：** 然后，你拿起这些分数，让“优化大师”帮你根据这些分数调整你的玩法，让它变得更好。
3.  **循环：** 既然你的玩法变了，新的玩法肯定有新的预期分数。所以你又把新的玩法交给“算分高手”去评估，然后“优化大师”又根据新的分数继续改进……

这个过程会一直循环下去，每次循环你的玩法都会变得更好一点点，直到有一天，“优化大师”发现：**“咦，根据现在这个预期分数，我的玩法已经不需要再改了！”**

当发生这种情况时，恭喜你，你已经找到了这个游戏的**最优玩法（最优策略）**，以及这个最优玩法下每个状态的**最高预期分数（最优价值函数）**！你就从一个普通玩家变成了游戏大师！

这就是 Policy Evaluation 和 Policy Improvement 的“舞蹈”，它们互相配合，最终帮助你找到游戏的最优解。

---
好的，我们继续用游戏的例子来区分 **策略改进 (Policy Improvement)** 和 **价值迭代 (Value Iteration)**。

在之前的讲解中，我们谈到了 **策略迭代** 这个大框架，它包含两个步骤：
1.  **策略评估 (Policy Evaluation)**：给定一个策略，算出每个状态的“预期分数”。
2.  **策略改进 (Policy Improvement)**：根据这些“预期分数”，调整策略，让它变得更好。

现在我们要深入一点，看看“策略改进”这个词在不同上下文中的含义，以及它与“价值迭代”的关系。

---

### Policy Improvement (策略改进) - 聚焦于“如何调整策略”

**Policy Improvement 主要是指“根据当前的价值函数，生成一个更好的策略”这个动作。**

当我们说“策略改进”时，通常指的是：

* **输入：** 你已经有了当前策略的价值函数 $V^\pi$（就是你“算分高手”算出来的每个状态的预期分数）。
* **动作：** 对于每个状态，你检查所有可能的行动。你不是看这个行动当前能给你多少奖励，而是看这个行动会把你带到哪个**下一个状态**，以及那个**下一个状态的“预期分数”**是多少。
* **决策：** 你选择那个能让你下一步进入“最好”（预期分数最高）状态的行动。
* **输出：** 一个新的、改进的策略 $\pi'$。

**这就像：** 你的“优化大师”拿到算分高手的报告后，他会逐个检查每个十字路口。在十字路口A，他会问：“如果我向左，会去到B点，B点的预期分数是80；如果我向右，会去到C点，C点的预期分数是50。” 于是，优化大师决定，在十字路口A，我就向左走。他把这个决策写进了新的“玩法指南”里。

所以，Policy Improvement 是一个**策略选择的过程**，它直接改变你玩游戏的“规则”（策略）。

---

### Value Iteration (价值迭代) - “一边算分一边改进玩法”的快捷方式

现在我们来看 **价值迭代 (Value Iteration)**。它不是策略迭代的某个步骤，而是**一种不同的、更直接的动态规划算法**，它把“策略评估”和“策略改进”这两个步骤**融合**在了一起。

**价值迭代的目的：** 直接找到每个状态的**最优预期分数**（$V^*$），然后从这个分数中“提取”出**最优玩法**（$\pi^*$）。

**它的核心思想是：**
在 Policy Iteration 中，你先让“算分高手”把当前玩法下的所有分数算完（可能要算很多轮），然后才让“优化大师”去调整玩法。

**Value Iteration 则是“急性子”：**
它不再等待每个状态的预期分数完全收敛，而是在每次更新一个状态的预期分数时，**它不是按照某个固定的策略去算下一个状态的平均分数，而是直接看所有可能的行动中，哪个行动能带来最大的预期分数。**

**这就像：**
你的“算分高手”和“优化大师”合二为一了。他现在在计算十字路口A的预期分数时，他不再问“按旧玩法我该怎么走？”，而是直接问：“在A点，我能走的路有左、右、上、下。如果我走左边，未来能得80分；走右边能得50分；走上面能得70分…… 那么，我当然要选择能让我得80分的左边！所以，A点的预期分数就暂时定为80分！”

**更新规则的变化：**
* **Policy Evaluation (迭代策略评估) 的更新是：** $V(S) \leftarrow \sum_A \pi(A|S) \sum_{S'} P(S'|S, A) [R + \gamma V(S')]$
    * 这里是**根据当前策略 $\pi$**，计算所有可能行动的加权平均。
* **Value Iteration (价值迭代) 的更新是：** $V(S) \leftarrow \max_A \sum_{S'} P(S'|S, A) [R + \gamma V(S')]$
    * 这里是**直接取所有可能行动中能导致最大预期回报的那一个。** 这就是在更新价值的同时，隐式地进行着策略改进。

**所以，Value Iteration 的特点是：**
* **一步到位：** 每次更新价值时，都直接考虑如何最大化未来的回报，而不是等待策略完全固定下来。
* **更高效：** 理论上，Value Iteration 可能需要更多的迭代次数来使价值函数收敛，但由于每次迭代的计算更简单（只是一个 max 操作和一次 Bellman 更新，而不是多次 Bellman 更新直到策略评估收敛），它在实践中通常比 Policy Iteration 更快地找到最优解。
* **最终结果：** 它最终会收敛到**最优价值函数** $V^*$。一旦有了 $V^*$，你再进行一次简单的 Policy Improvement 步骤（即在每个状态下选择能使 $Q^*(s,a)$ 最大的行动），就能得到**最优策略** $\pi^*$。

---

### 总结区别：

| 特性           | Policy Improvement (作为策略迭代的一部分)                               | Value Iteration (一种独立的 DP 算法)                                 |
| :------------- | :---------------------------------------------------------------------- | :---------------------------------------------------------------------- |
| **关注点** | 根据当前价值函数，**调整策略本身** | **直接更新价值函数**，同时隐式地寻找最优行动                          |
| **更新方式** | 对每个状态，**明确选择一个能最大化 Q 值的行动作为新策略** | 对每个状态，**直接将价值更新为所有行动中能获得的最大预期回报** |
| **与策略关系** | **明确地修改策略**，使得策略朝着贪婪方向变化                          | **不显式维护策略**，策略的最佳选择隐含在价值更新中，最后再提取出来 |
| **迭代次数** | 每次策略改进，通常需要**完整的策略评估**（多次价值更新）               | 每次迭代只进行**一次价值更新**（包含了对最佳行动的选择）             |
| **收敛** | 每次改进都会生成一个更好的策略，直到策略稳定为最优策略                  | 价值函数直接收敛到最优价值函数 $V^*$，再从中提取最优策略 $\pi^*$      |

可以这样理解：

* **Policy Improvement** 是你在做一道填空题：“在A状态，我应该选择哪个行动？”
* **Value Iteration** 更像是你在做一道选择题：“在A状态，我未来能获得的最大分数是多少？（这个分数决定了你下一步该怎么走）”

两者最终都能找到最优解，但实现路径和计算侧重点不同。
---
好的，我们再来类比解释一下 **同步动态规划 (Synchronous Dynamic Programming)** 和 **异步动态规划 (Asynchronous Dynamic Programming)**。

想象你正在管理一个大型仓库，里面有很多货架（每个货架可以看作一个“状态”），每个货架上都放着不同价值的货物。你的目标是优化货物的摆放，让整个仓库的效率最高（即找到最优价值和最优策略）。

---

### 同步动态规划 (Synchronous Dynamic Programming)：“全盘清点，统一更新”

**同步动态规划就像你在做一次“全仓库大盘点”。**

* **流程：**
    1.  你先召集所有的盘点员。
    2.  你告诉他们：“现在，你们各自去清点自己负责的区域（货架），并记录下每个货架的**旧价值**。”
    3.  所有的盘点员都完成记录后，你再根据这些旧价值，统一计算出每个货架的**新价值**。
    4.  然后，在所有人都计算出新价值后，**同时**把这些新价值更新到货架上。
    5.  重复这个过程，直到所有货架的价值都稳定下来。

* **特点：**
    * **“步调一致”：** 在每一次“迭代”（一次大盘点）中，所有货架的价值都是基于**上一轮**的旧价值来计算的。没有哪个货架会提前使用它自己的最新计算结果。
    * **“全面性”：** 每次迭代都必须遍历并更新所有的货架。
    * **例子：** 我们之前讲的“策略迭代”和“价值迭代”，如果它们在每次循环中都遍历并更新所有的状态，那么它们就是同步动态规划的例子。

* **优点：** 概念简单清晰，容易理解和证明其收敛性。
* **缺点：** 如果你的仓库非常非常大，货架非常多，那么每次“全盘清点”都会非常耗时。你必须等待所有人都算完，才能进行下一步更新，效率可能不高。

---

### 异步动态规划 (Asynchronous Dynamic Programming)：“灵活清点，即时更新”

**异步动态规划就像你雇佣了一群“灵活机动的盘点员”。**

* **流程：**
    1.  盘点员们不再需要等待所有人，他们可以自由选择去清点哪个货架。
    2.  一个盘点员可能去清点货架A，他根据这个货架周围（相邻）货架的**最新价值**（可能是旧的，也可能是其他盘点员刚刚更新的）立即计算出货架A的**新价值**，然后**立即更新**货架A的价值。
    3.  紧接着，另一个盘点员可能去清点货架B，他也会使用货架A刚刚更新的那个最新价值（如果货架A是B的邻居的话）来计算货架B的新价值，并立即更新。
    4.  盘点员们以**任意顺序**、**任意频率**去清点和更新货架，只要最终每个货架都有机会被清点和更新到就行。

* **特点：**
    * **“非同步”：** 没有固定的迭代边界，每个货架的更新都是独立的，可以随时进行。
    * **“即时性”：** 在计算某个货架的新价值时，可以直接使用其相邻货架**最新可用的价值**（即使这个价值是在当前“迭代”中刚刚更新的）。
    * **“选择性”：** 可以选择性地更新重要的货架，或者那些价值变化大的货架，而不需要每次都遍历所有货架。

* **优点：**
    * **效率高：** 在很多情况下，异步方法可以更快地收敛。因为信息（价值变化）可以更快地在货架之间传播。你不需要等待整个仓库盘点完，就可以立即利用最新的信息。
    * **灵活性：** 可以根据实际情况或算法需求，设计不同的更新顺序和策略。
* **缺点：** 概念和实现可能比同步方法复杂，要确保收敛性也需要一些条件（例如，每个货架最终都必须被更新到）。

---

### 总结：

| 特性           | 同步动态规划 (Synchronous DP)                                     | 异步动态规划 (Asynchronous DP)                                  |
| :------------- | :------------------------------------------------------------------ | :---------------------------------------------------------------- |
| **更新方式** | 每次迭代都**全面扫描**所有状态，使用**上一轮**的价值计算新价值，然后**同时更新** | **不定期、不按序**更新状态，使用**最新可用**的价值计算新价值，然后**立即更新** |
| **信息传播** | 价值信息在一轮迭代结束后“集体”传播                                  | 价值信息可以“实时”或更频繁地传播                                |
| **主要优点** | 概念简单，易于实现和理论分析                                        | 潜在的效率提升，尤其适用于大规模问题，更灵活                    |
| **主要缺点** | 每次迭代开销大，可能效率低下                                        | 实现更复杂，收敛性分析更复杂                                    |

简单来说：
* **同步动态规划** 就像“期末统考”，考完所有科目才出总成绩。
* **异步动态规划** 就像“随堂测验”，考完一科立刻出成绩，并且下一次测验可能马上就用上这个成绩。

在实际应用中，尤其是在状态空间非常大的情况下，异步动态规划由于其更高的效率和灵活性，经常被采用。

---
“Equiprobable random policy” 在强化学习中指的是一种每个可能行动被选择的概率都相等的策略。
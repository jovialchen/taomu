---
title: "Transformers的attention为什么要除以${d_k}$开方?"
layout: post
date: 2025-02-25
categories: tech_coding
tags:
    - LLM
---


一些关于transfomers的思考. 谢谢LLM解答.
在计算 $$QK^T$$时，点积的值会随着向量维度 $$d_k$$的增大而变大。这会导致：
- **数值过大：** 使得 softmax 的输入值非常大，导致 softmax 输出接近于 one-hot 分布。
- **梯度问题：** 饱和的 softmax 会使梯度变得很小，训练过程变得不稳定。

通过除以 $$\sqrt{d_k}$$，可以将点积缩放到一个较为适中的范围，从而使 softmax 的梯度更加平滑，训练更容易收敛。
---
title: "多头注意力的维度是如何做到的?"
layout: post
date: 2025-02-21
categories: tech_coding
tags:
    - machine_learning, LLM
---

最近一直在学习自然语言处理和大语言模型的一些基本概念, 这些内容是在ChatGPT, Deepseek等大语言模型的帮助下记录的。


在 **多头注意力机制（Multi-Head Attention, MHA）** 中，核心思想是将输入的特征向量拆分成多个子空间（多个“头”）进行并行注意力计算，然后再将结果拼接起来。这种方式可以让模型在不同的表示子空间中学习不同的注意力模式。

---

### **1️⃣ 每个头的维度计算**

假设输入的隐藏向量维度为 $d_{\text{model}}$，注意力头的数量为 $h$。  

**每个头的维度 $d_k$ 的计算公式为：**

$$
d_k = \frac{d_{\text{model}}}{h}
$$

通常 $d_{\text{model}}$ 可以被 $h$ 整除（否则会导致维度对不齐的问题）。  

**举个例子：**
- 如果 $d_{\text{model}} = 512$ 且 $h = 8$，那么每个头的维度 $d_k = \frac{512}{8} = 64$。

---

### **2️⃣ 整个注意力层输出维度**

整个多头注意力层的输出维度是 **$d_{\text{model}}$**，和输入维度保持一致。  

**流程：**
1. 输入向量通过线性变换生成 $h$ 个不同头的 Query、Key 和 Value，形状为：
   - $Q, K, V \in \mathbb{R}^{\text{batch\_size} \times \text{seq\_length} \times d_k}$
2. 对每个头独立计算 **Scaled Dot-Product Attention**。
3. 将所有 $h$ 个头的输出拼接（concatenate）在一起，形状为：
   - $\mathbb{R}^{\text{batch\_size} \times \text{seq\_length} \times (h \cdot d_k)}$
4. 最后再通过一个线性变换投影回 $d_{\text{model}}$ 维度。

**继续刚才的例子：**
- 输入：$\mathbb{R}^{\text{batch\_size} \times \text{seq\_length} \times 512}$
- 每个头的输出：$\mathbb{R}^{\text{batch\_size} \times \text{seq\_length} \times 64}$
- 拼接后：$\mathbb{R}^{\text{batch\_size} \times \text{seq\_length} \times 512}$
- 最终输出维度保持为 $512$

---

### ✅ **总结**

| 参数          | 公式                           | 举例数值（$d_{\text{model}}=512, h=8$） |
|---------------|--------------------------------|---------------------------------------------|
| 每个头的维度  | $d_k = \frac{d_{\text{model}}}{h}$ | 64                                          |
| 输出维度      | $d_{\text{model}}$         | 512                                         |

需要注意的一点是，多头注意力机制的核心优势在于它能够从不同子空间捕捉多样化的信息，这对于捕捉序列中的复杂关系非常重要。  


https://jalammar.github.io/illustrated-transformer/

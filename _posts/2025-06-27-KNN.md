---
title: "KNN"
layout: post
date: 2025-06-27
categories: tech_coding
tags:
    - machine_learning
---


K-近邻（K-Nearest Neighbors，简称 KNN）算法是一种非常简单但有效的**非参数**、**监督学习**算法，可以用于**分类**和**回归**任务。它的核心思想是：**“物以类聚，人以群分”**。一个数据点的类别或值由其最近的 K 个邻居的类别或值来决定。

---

## KNN 的核心思想

KNN 是一种**“惰性学习”（Lazy Learning）**算法，因为它在训练阶段不做任何模型构建工作，而是直接存储所有的训练数据。所有的计算和学习都发生在**预测阶段**。

### 工作原理

假设我们有一个新的、未分类的数据点，我们想知道它属于哪个类别（分类问题）或者它的值是多少（回归问题）。KNN 算法会执行以下步骤：

1.  **选择 K 值：** 首先，你需要决定一个整数 `K`，它代表在做预测时要考虑的最近邻居的数量。`K` 是一个超参数，它的选择对模型性能有很大影响。
    * 如果 `K=1`，那么新的数据点将被分配给它最近的那个训练样本的类别。
    * 如果 `K` 较大，它会考虑更多的邻居，这可以使决策边界更平滑，但可能导致模型忽略局部细节。

2.  **计算距离：** 计算新的数据点与训练数据集中**所有**数据点之间的距离。常用的距离度量包括：
    * **欧几里得距离 (Euclidean Distance)：** 最常见的距离，就是我们平时所说的两点之间直线距离。
        $d(\mathbf{p}, \mathbf{q}) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}$
    * **曼哈顿距离 (Manhattan Distance / City Block Distance)：** 像在城市街区行走，只能沿着网格线移动。
        $d(\mathbf{p}, \mathbf{q}) = \sum_{i=1}^{n} |p_i - q_i|$
    * **闵可夫斯基距离 (Minkowski Distance)：** 欧几里得距离和曼哈顿距离的泛化。
    * **汉明距离 (Hamming Distance)：** 用于比较类别型数据。

3.  **找到 K 个最近邻居：** 根据计算出的距离，选择离新数据点最近的 K 个训练样本。

4.  **进行预测：**
    * **对于分类任务：** 观察这 K 个最近邻居的类别。新的数据点将被分配到这 K 个邻居中**出现次数最多的类别**。为了避免平局，通常会选择奇数的 K 值。
    * **对于回归任务：** 计算这 K 个最近邻居的**目标值的平均值**（或中位数）。这个平均值就是新数据点的预测值。

### 示例（分类）

想象你在一个公园里，有一些蓝色和红色的花。你发现一朵新的绿色花，想知道它是蓝色花还是红色花。

* 如果你选择 K=3：你找到离绿色花最近的 3 朵花。如果其中 2 朵是蓝色，1 朵是红色，那么你预测这朵绿色花是蓝色。
* 如果你选择 K=5：你找到离绿色花最近的 5 朵花。如果其中 3 朵是红色，2 朵是蓝色，那么你预测这朵绿色花是红色。

---

## KNN 的特点

* **非参数 (Non-parametric)：** KNN 不对数据的底层分布做任何假设，这使得它对数据分布的形状非常灵活。
* **惰性学习 (Lazy Learning)：** 它没有明确的训练阶段来构建一个模型。模型实际上是整个训练数据集本身。所有计算都在预测时完成。
* **基于实例的学习 (Instance-based Learning)：** 预测是直接基于存储的训练实例进行的。

---

## KNN 的优缺点

### 优点：

* **简单易懂：** 算法概念非常直观，容易实现和理解。
* **无需训练阶段：** 由于是惰性学习，没有显式的模型训练时间。所有数据在预测时才进行处理。
* **适用于多分类问题：** 可以直接用于多类别分类，无需像 SVM 或逻辑回归那样进行 OvA 或 OvO 转换。
* **处理非线性问题：** 能够捕捉数据中的复杂非线性关系，因为它没有对决策边界做任何假设。
* **易于增加新数据：** 当有新的训练数据可用时，只需将其添加到数据集中即可，无需重新训练模型。

### 缺点：

* **计算成本高（预测阶段）：** 对于大型数据集，每次预测都需要计算新数据点与所有训练数据点之间的距离，这可能非常耗时和计算密集。
* **内存需求高：** 需要存储整个训练数据集，因为在预测时需要访问所有数据。
* **对特征缩放敏感：** 由于基于距离的度量，不同尺度的特征会严重影响距离计算。因此，通常需要对数据进行标准化或归一化。
* **“维度诅咒”（Curse of Dimensionality）：** 在高维空间中，数据点之间的距离变得不那么有意义，所有的点都倾向于彼此“远离”，导致 KNN 的性能急剧下降。
* **对异常值和噪声敏感：** 如果训练数据中包含噪声或离群点，它们可能会被选为最近邻居，从而对预测结果产生负面影响，尤其是在 K 值较小的时候。
* **类别不平衡问题：** 如果数据集中某个类别的样本数量远远多于其他类别，KNN 可能会偏向于预测数量多的类别。

---

## K 值的选择

选择一个合适的 K 值是使用 KNN 的关键。

* **K 值过小：** 模型会过于敏感，容易受到噪声和异常值的影响，导致**过拟合**。决策边界会非常不规则。
* **K 值过大：** 模型会变得过于平滑，可能会错过数据中的重要模式，导致**欠拟合**。预测会趋于训练数据的整体平均。

**选择 K 值的一些常见方法：**

* **经验法则：** 有些人建议 K 可以取 $\sqrt{N}$（N 是训练样本数），或者选择一个较小的奇数（如 3, 5, 7）。但这只是一个起点。
* **交叉验证 (Cross-validation)：** 这是最推荐的方法。通过在不同的 K 值下运行交叉验证，选择在验证集上表现最好的 K 值。
* **错误率曲线：** 绘制不同 K 值对应的错误率，找到错误率开始显著下降后趋于平稳的“肘部”点。

---

## 应用场景

KNN 尽管有缺点，但由于其简单性，在某些场景下仍然是一个有用的算法：

* **推荐系统：** 根据用户行为的相似性推荐商品或内容。
* **图像识别：** 简单的图像分类。
* **手写数字识别：** MNIST 数据集上的早期应用。
* **异常检测：** 识别与大多数数据点相距较远的点。
* **基因表达分析：** 在生物学中对基因进行分类。

由于其计算效率问题，KNN 在处理非常大的数据集或高维数据时，通常不如更复杂的算法（如支持向量机、决策树或集成方法）表现好。但在小到中等规模的数据集上，它仍然是一个值得尝试的基线模型。

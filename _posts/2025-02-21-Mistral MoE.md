---
title: "大模型的参数量是如何计算出来的?"
layout: post
date: 2025-02-21
categories: tech_coding
tags:
    - LLM
    - Transformers
---


最近一直在学习自然语言处理和大语言模型的一些基本概念, 这些内容是在ChatGPT, Deepseek等大语言模型的帮助下记录的。

### 📜 **Mistral 的 Mixture of Experts (MoE) 结构总结**

---

#### 🔧 **1. MoE 结构核心概念**

- **MoE（Mixture of Experts）** 是一种稀疏激活机制，在每层 Transformer 中引入多个独立的 **专家（Experts）**，通常是 8、16 或更多。  
- 每个专家是一个 **独立的前馈网络（Feedforward Network, FFN）**，具有类似于标准 Transformer 的结构（Linear → Activation → Linear）。  

---

#### 🔍 **2. 处理流程**

1. **输入 Token**  
   - 每一层接收来自上一层的输出作为输入。  

2. **Gating 机制选择专家**  
   - 一个小型 **gating 网络** 会为输入 token 计算分数。  
   - 根据分数选择 **Top-k**（通常是 2）个最合适的专家进行前向传播。  

3. **专家前向传播（Forward Pass）**  
   - 被选中的专家对输入 token 进行独立的前馈计算。  

4. **输出融合（Aggregation）**  
   - 将多个专家的输出按 gating 权重加权融合，生成当前层的最终输出。  

5. **负载平衡（Load Balancing）**  
   - 在训练时添加额外的损失项，确保专家被均匀使用，避免部分专家过载。  

---

#### 🔄 **3. 专家选择是逐层独立进行的**

- 每一层 Transformer Block 都有自己的 **gating 网络** 和 **专家池**。  
- 在每一层中，输入会重新通过 gating 网络选择专家，**不同层可能会激活不同的专家**，即使是相同的 token。  
- 这种层层动态专家选择机制使模型能够学习到**更加多样化和复杂的特征**。  

---

#### 🚀 **4. Mistral MoE 的优势**

- ✅ **计算高效**：每个 token 只激活少数专家，减少了计算开销。  
- ✅ **扩展性强**：可以增加专家数量而不显著增加推理成本。  
- ✅ **性能出色**：在大规模语言建模任务上，MoE 通常比密集模型表现更好。  
- ✅ **动态适应**：不同 token 和层次可根据输入动态选择最适合的专家。  

---

#### 🏗️ **5. 例子（每层独立专家选择流程）**

| 层数 | Gating 选择的专家 | 被激活的专家 | 层输出传递 |
|-----|----------------|-----------|-------------|
| 第 1 层 | 根据输入打分 | Expert 3 和 Expert 2 | 输出传递给第 2 层 |
| 第 2 层 | 根据新输入重新打分 | Expert 1 和 Expert 4 | 输出传递给第 3 层 |
| 第 3 层 | 根据新输入重新打分 | Expert 2 和 Expert 5 | 输出传递给下一层 |

---

#### 🛠️ **6. 关键要点总结**

- 🔄 **每层独立专家选择**，基于输入 token 的特征动态决定。  
- 🎯 **稀疏激活**，每次只选择部分专家参与计算（比如 2 个）。  
- ⚖️ **负载平衡机制**，保证所有专家被均匀利用。  
- ⚡ **高效且强大**，在大规模任务中兼顾性能和效率。  


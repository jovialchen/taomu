---
title: "大模型的参数量是如何计算出来的?"
layout: post
date: 2025-02-21
categories: tech_coding
tags:
    - LLM
---

最近一直在学习自然语言处理和大语言模型的一些基本概念, 这些内容是在ChatGPT, Deepseek等大语言模型的帮助下记录的。

下面给出一个详细的计算过程，以 Llama 3.1 8B 为例说明大模型参数数量的估算方法。注意实际实现中可能有细微差异，但大致流程如下：

---

### 1. 模型配置（以 Llama 3.1 8B 为例）
- **隐藏层维度（d）**：4096  
- **层数（L）**：32  
- **注意力头数（n_heads）**：32  
  - 则每个头的维度：  
    $$
    d_{head} = \frac{4096}{32} = 128
    $$
- **KV 头数（n_kv_heads）**：8  
  - 这意味着查询有 32 个头，而键和值只有 8 个头（每个键/值头服务 4 个查询头）。
- **词汇表大小（vocab_size）**：128,256

---

### 2. 各部分参数计算

#### (1) **词嵌入层**  
模型需要一个大小为  
$$
\text{Embedding参数} = \text{vocab_size} \times d = 128\,256 \times 4096
$$
约为 **5.25 × 10^8**（5.25 亿）参数。

---

#### (2) **每个 Transformer 层**

一个标准的 Transformer 解码器层主要由两部分组成：自注意力模块和前馈网络（FFN）。

**a. 自注意力模块**  
在 Llama 系列中使用的是分组查询注意力（GQA），具体计算如下：

- **查询矩阵 Q**：  
  权重尺寸为  
  $$
  d \times (n_heads \times d_{head}) = 4096 \times (32 \times 128) = 4096 \times 4096
  $$
  参数数：约 16,777,216。

- **键矩阵 K**：  
  尺寸为  
  $$
  d \times (n_kv_heads \times d_{head}) = 4096 \times (8 \times 128) = 4096 \times 1024
  $$
  参数数：约 4,194,304。

- **值矩阵 V**：同键矩阵，也是约 4,194,304 参数。

- **输出投影矩阵**：  
  尺寸为  
  $$
  (n_heads \times d_{head}) \times d = 4096 \times 4096
  $$
  参数数：约 16,777,216。

因此，自注意力模块总参数约为：
$$
16,777,216 + 4,194,304 + 4,194,304 + 16,777,216 \approx 41,943,040
$$

---

**b. 前馈网络（FFN）**  
Llama 采用了 SwiGLU 变体。其基本形式为：
$$
\text{FFN}_{\text{SwiGLU}}(x) = \left(\text{SiLU}(xW_1) \odot (xW_3)\right) W_2
$$
其中有三个线性层：

- 假设标准的扩展因子为 4，则中间维度 $$d_{ff} = 4 \times d = 4 \times 4096 = 16\,384$$（注意：为了保持计算量一致，实际实现中 SwiGLU 可能会把该值缩小约 2/3，但如果不缩小，参数数目会更接近 8B 模型的规模）。
- 因此：
  - **W1**：尺寸 $$4096 \times 16\,384$$，参数约 67,108,864  
  - **W3**：同上，也约 67,108,864  
  - **W2**：尺寸 $$16\,384 \times 4096$$，参数约 67,108,864  

FFN 模块总参数：
$$
3 \times 4096 \times 16\,384 \approx 201,326,592
$$

---

**c. 单层 Transformer 层总参数**  
将注意力和 FFN 参数相加：
$$
41,943,040 + 201,326,592 \approx 243,269,632
$$

---

#### (3) **32 层 Transformer 模块**  
32 层的参数总数为：
$$
32 \times 243,269,632 \approx 7.78 \times 10^9 \quad (\sim7.78 \text{ 亿})
$$

---

#### (4) **最后的归一化层和输出投影**  
通常会有一个层归一化层（参数量非常小，约 4096）以及语言模型头。如果输出头与词嵌入共享权重（即权重绑定），则不额外增加参数；否则还需加上一个与词嵌入层相同规模的矩阵（约 5.25 亿参数）。

---

### 3. **总参数估算**

- **词嵌入层**：约 525M  
- **Transformer 层（32 层）**：约 7.78B  
- **归一化层和（可能的）输出投影**：如果权重绑定，则几乎不增加；如果不绑定，则增加约 525M。

将以上数值相加，总数大致为：
$$
525\,\text{M} + 7,780\,\text{M} \approx 8,305\,\text{M}
$$
即约 **8.3B** 参数，这与“8B 模型”的名称基本吻合（在实际发布时，可能会对一些细节做调整，使参数数量更精确地控制在 8B 左右）。

---

### 总结

1. **嵌入层**：$$\text{vocab_size} \times d $$
2. **每层 Transformer**：
   - 自注意力：计算 Q、K、V 以及输出投影，共约 41.94M 参数。
   - 前馈网络：采用 SwiGLU 结构，共约 201.33M 参数。
   - 单层总计约 243.27M 参数。
3. **所有层**：32 层乘以单层参数约 7.78B。
4. **加上嵌入和（可能的）输出投影**，总参数约 8.3B。

这种方法就是把模型各部分（嵌入、自注意力、FFN、归一化、输出层）的参数逐一计算，再求和得到总体参数量。虽然实际实现可能在细节上有所不同（例如是否使用偏置、是否绑定权重等），但这种粗略估算方法可以帮助我们理解大模型的参数规模。

---

以上便是以 Llama 3.1 8B 为例，如何计算大模型参数量的一个详细说明。 

参考资料包括知乎专栏关于 Llama 参数计算的讨论以及各大博客的技术解析。  citeturn0search3


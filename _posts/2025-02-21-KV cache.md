---
title: "KV Cache"
layout: post
date: 2025-02-21
categories: tech_coding
tags:
    - machine_learning, LLM
---

最近一直在学习自然语言处理和大语言模型的一些基本概念, 这些内容是在ChatGPT, Deepseek等大语言模型的帮助下记录的。


在 **Transformers** 模型中，**KV cache**（Key-Value Cache）是一个在推理（inference）阶段常用的优化技术，主要用来加速 **自回归生成**（例如在 GPT、LLM 等模型生成文本时）。  

---

### 🔍 **KV Cache 的核心概念**

在 Transformer 的 **自注意力机制**（Self-Attention）中，每一层都会从输入生成三个向量：  
- **Query (Q)**  
- **Key (K)**  
- **Value (V)**  

计算注意力时，主要用公式：  
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

---

### 🚀 **为什么需要 KV Cache？**

在生成文本时，Transformers 通常是 **逐步生成**（即一次预测一个 token）。如果不使用缓存，每生成一个新 token，模型都需要重新计算所有之前 token 的注意力，计算复杂度为 **O(N²)**（N 是序列长度）。

**KV Cache 的作用** 是在生成过程中，**缓存之前生成 token 的 K 和 V 值**，下次生成新 token 时，只需要计算新的 Q 与缓存的 K、V 进行注意力计算。这样避免了重复计算，显著提高了推理速度。  

---

### 📊 **KV Cache 的实际流程**

1. **第一次推理**：输入前几个 token，计算出对应的 K 和 V，存入缓存。  
2. **生成新 token**：
   - 只计算新 token 的 Q、K、V。
   - 用新的 Q 和之前缓存的 K、V 计算注意力。
   - 将新的 K、V 追加到缓存中。
3. **重复以上步骤**，直到生成结束。

---

### 💡 **在代码中的体现（以 PyTorch 为例）**

```python
# 简化的 KV Cache 逻辑
# 假设 cached_k 和 cached_v 是之前的缓存
new_k, new_v = model.compute_kv(new_input)

# 拼接缓存
cached_k = torch.cat([cached_k, new_k], dim=1)
cached_v = torch.cat([cached_v, new_v], dim=1)

# 用缓存和新 Q 计算注意力
attention_scores = torch.matmul(query, cached_k.transpose(-2, -1))
```

---

### ⚙️ **KV Cache 的优势**

- ✅ **减少重复计算**：尤其在长序列生成时，速度提升显著。  
- ✅ **降低内存占用**：不需要反复存储和计算整个序列的 K、V。  
- ✅ **广泛应用**：GPT 系列、Llama、BERT 推理加速等都使用了这种缓存机制。  


## 为什么只有KV有Cache 但是Q没有?
在 Transformer 模型中，KV cache 只缓存 Key（K）和 Value（V），而不缓存 Query（Q），主要有以下原因：

1. **自回归生成的特点**  
   在自回归生成中，每一步生成一个新 token 时，只需计算该 token 对应的 Q，而之前 token 的 K 和 V（它们代表之前已生成的上下文信息）在后续每一步都会被重复利用。因此，将 K 和 V 缓存下来可以避免重复计算，显著加速推理。

2. **Q 的一次性使用性**  
   当前步骤生成的新 token的 Q 只在当下用来与缓存中的 K 进行注意力计算，下一步生成 token时不会再使用该 Q，所以没有必要缓存。也就是说，每个时间步生成的 Q 只参与一次计算，缓存 Q 无法带来重复利用的好处。

3. **内存和计算资源的平衡**  
   缓存 K 和 V 能够大幅度降低计算开销，因为对于长序列来说，重复计算 K 和 V 会非常昂贵；而缓存 Q 既不带来计算节省，又会额外占用内存资源，因而没有实际收益。

综合来说，由于自回归生成时只有当前 token 的 Q 被使用，而之前 token 的 K 和 V在每一步都需要被用到，所以设计上只缓存 K 和 V 即可实现高效推理。

这也是许多工程实践中（例如 GPT 系列、Llama 等模型）采用 KV cache 技术的主要原因。  


https://medium.com/@joaolages/kv-caching-explained-276520203249
---
title: "为什么attention要用Q, K, V三个矩阵, 而不是同一个."
layout: post
date: 2025-03-24
categories: tech_coding
tags:
    - Transformers
---


又是一个有什么问题不懂就问本人系列.

---

在**Transformer** 结构中，输入需要被转换成 **Query (Q)**、**Key (K)** 和 **Value (V)** 三个不同的矩阵，而不是使用同一个矩阵，主要是为了**实现自注意力机制 (Self-Attention)**，从而有效地计算不同词之间的相关性。  

### 1. **Q、K、V 的作用**
在自注意力机制 (Self-Attention) 中，每个 token 都需要和其他 token 进行交互。为了实现这个交互，Transformer 将输入向量 $X$ 通过不同的 **可学习权重矩阵** 变换成：
- **Query (Q)**：表示当前 token 作为**查询** (query) 时的特征。  
- **Key (K)**：表示当前 token 作为**键** (key) 时的特征。  
- **Value (V)**：表示当前 token 作为**值** (value) 时的特征。

这种划分的作用是：
- **Q 和 K 计算注意力权重**：$Q$ 与 $K$ 进行点积计算，得到**相关性得分 (attention scores)**，然后通过 softmax 归一化，得到最终的注意力权重。  
- **V 负责信息聚合**：注意力权重再与 $V$ 进行加权求和，生成最终的表示。

公式如下：
$$
\text{Attention}(Q, K, V) = \text{softmax} \left(\frac{Q K^T}{\sqrt{d_k}}\right) 
$$

### 2. **为什么不用同一个矩阵？**
如果 Q、K、V 只是同一个矩阵（即直接用输入 $X$），会导致以下问题：

1. **信息混淆，无法进行不同角色的计算**
   - **Q 需要查询信息**，用于计算当前 token 和其他 token 的关系。  
   - **K 需要存储可被查询的信息**，以便 Q 计算注意力权重。  
   - **V 需要存储内容信息**，最终用于聚合生成新的特征表示。  
   - 如果 Q=K=V，那么输入数据就没有进行不同的投影变换，注意力机制就会变成一个简单的加权求和，无法有效学习不同 token 之间的复杂关系。

2. **无法自适应学习不同的关注方式**
   - 通过使用**不同的投影矩阵**（$W_Q$、$W_K$、$W_V$），Transformer 允许 Q、K、V 具有不同的特性，这样可以让模型学习更灵活的注意力模式，比如：
     - 词语的不同语境表示
     - 关注全局 vs 局部信息
     - 多头注意力机制中不同的头学习不同的特征

3. **点积计算的数值稳定性**
   - 直接对原始输入做点积可能导致数值过大或过小，影响梯度稳定性。而单独的线性变换（$W_Q, W_K, W_V$）可以通过训练调整不同投影的尺度，使得注意力计算更加稳定。

### 3. **总结**
Q、K、V 三者的划分让 Transformer 能够：
✅ 计算不同 token 之间的关系（Q 和 K）  
✅ 通过注意力权重聚合信息（V）  
✅ 使得模型可以学习更复杂的表示能力，而不仅仅是简单的加权求和  

这样，Transformer 就能更好地理解文本中的依赖关系，并提高模型的表达能力！ 🚀
---
type: Supporting

---

# 3. 随机森林回归


随机森林回归器 (Random Forest Regressor) 是**集成学习**（Ensemble Learning）领域中一种非常强大和流行的算法。它基于**决策树回归器**，但通过组合多棵决策树来显著提高模型的性能和鲁棒性。

### 核心思想：“集体的智慧”

随机森林的核心思想是“集体的智慧优于个体”。它不是只训练一棵决策树，而是训练**多棵**（通常是几百棵甚至几千棵）独立的决策树，然后将这些树的预测结果进行**平均**，从而得到最终的预测值。

### 随机森林回归器的工作原理

随机森林通过引入**随机性**来确保每棵树都是不同的，从而避免了单棵决策树容易过拟合和不稳定的问题。这种随机性主要体现在两个方面：

1.  **数据采样（行采样）：Bagging (Bootstrap Aggregating)**
    * 随机森林使用一种称为 **Bagging** 的技术。
    * 对于森林中的每棵决策树，它都会从原始训练数据集中**有放回地随机抽取**一部分样本（通常是与原始数据集大小相同的样本量）。这意味着有些样本可能会被多次抽取，有些样本可能根本不会被抽取到。
    * 这种采样方式确保了每棵树都看到了略微不同的训练数据子集，从而导致每棵树的结构和预测结果都不同。

2.  **特征选择（列采样）：**
    * 在每棵树的每个分裂节点上，随机森林不会考虑所有可用的特征来寻找最佳分裂点。
    * 相反，它会**随机选择一个子集的特征**（例如，如果总共有100个特征，每次只随机选择10个特征来考虑），然后只从这部分特征中寻找最佳分裂点。
    * 这种随机性进一步增加了树之间的多样性，减少了树之间的相关性。

### 训练和预测过程

1.  **训练阶段：**
    * **创建多棵决策树：** 根据你设定的树的数量（`n_estimators` 参数），随机森林会重复以下步骤 `n_estimators` 次：
        * 从原始训练数据集中**有放回地随机抽取**一个子集（Bagging）。
        * 从所有特征中**随机选择一个子集**。
        * 使用这个随机抽取的数据子集和随机选择的特征子集，**训练一棵决策树回归器**。每棵树都会独立地学习数据中的模式，并根据其叶子节点中样本的平均值进行预测。
    * 最终，你得到一个由多棵独立训练的决策树组成的“森林”。

2.  **预测阶段：**
    * 当有一个新的数据点需要预测时：
        * 将这个数据点输入到森林中的**每一棵**决策树中。
        * 每棵树都会输出一个预测的连续数值。
        * **平均所有树的预测结果：** 将所有决策树的预测值进行**平均**，得到最终的随机森林回归器的预测值。

### 随机森林回归器的优缺点

**优点：**

* **高准确性：** 通过结合多棵树的预测，随机森林通常比单棵决策树具有更高的预测准确性。
* **鲁棒性强：** 对异常值和噪声具有较强的抵抗力，因为单个异常值对整个森林的影响较小。
* **不容易过拟合：** 引入的随机性（数据采样和特征采样）有效地降低了模型的方差，使其不容易过拟合训练数据。
* **特征重要性评估：** 随机森林可以评估每个特征对预测的重要性，这有助于理解数据和进行特征选择。
* **处理高维数据：** 能够很好地处理包含大量特征的数据集。
* **并行化：** 森林中的每棵树都可以独立训练，因此训练过程可以并行化，从而提高效率。

**缺点：**

* **可解释性相对较差：** 相比于单棵决策树，随机森林是一个“黑盒”模型，很难直观地理解其内部的决策过程。虽然可以提供特征重要性，但无法像单棵树那样直接展示决策路径。
* **计算成本较高：** 需要训练大量的决策树，因此训练时间和存储空间可能比单棵决策树大。
* **无法外推：** 和单棵决策树一样，随机森林也无法预测超出训练数据范围的值。
* **对稀疏数据表现不佳：** 在处理高度稀疏的数据时，可能不如线性模型或支持向量机。

### 实际应用

随机森林回归器广泛应用于各种需要预测连续值的场景，包括：

* **房价预测**
* **股票价格预测**
* **销售额预测**
* **温度预测**
* **生物医学中的各种连续指标预测**

总的来说，随机森林回归器是一种非常实用且性能优异的算法，在许多机器学习竞赛和实际应用中都取得了很好的效果。它通过集成和随机化有效地解决了单棵决策树的过拟合和不稳定性问题。


---
title: "大语言模型为什么只有Decoder?"
layout: post
date: 2025-02-21
categories: tech_coding
tags:
    - LLM
    - Transformers
---

最近一直在学习自然语言处理和大语言模型的一些基本概念, 这些内容是在ChatGPT, Deepseek等大语言模型的帮助下记录的。


这是个很好的问题！像 GPT 这样的 **大语言模型（LLM）** 只使用 **decoder** 的架构，主要是因为它们的核心任务是 **生成文本**。要理解为什么只用 decoder，我们可以从 **Transformer 架构** 的原理说起。

---

### 🔍 **Encoder 和 Decoder 的区别**

在最初的 Transformer 架构中（由 Vaswani 等人在 2017 年提出），模型包括两个部分：

- **Encoder**：主要负责理解和提取输入信息的特征（比如在机器翻译中，把英文句子转换成一个语义表示）。Encoder 结构适合做分类、摘要、情感分析等理解任务。

- **Decoder**：根据编码器输出的信息生成新内容（比如根据语义表示生成对应的法语句子）。Decoder 特别擅长文本生成任务。

---

### 💡 **为什么 GPT 只用 Decoder？**

1. **自回归生成（Autoregressive Generation）**  
   GPT 的核心任务是 **根据已有的上下文生成下一个单词**，也就是自回归生成。这种生成方式天然适合使用 **decoder-only** 结构，因为 decoder 会在每个时间步只看到之前的单词，然后预测下一个单词。

2. **注意力机制的掩码（Causal Masking）**  
   Decoder 在生成时使用了 **masked self-attention**，确保模型不能看到未来的单词（避免“作弊”）。比如，如果你让 GPT 生成“我今天很”，它只能根据“我今天很”来猜下一个字，不能提前看到“开心”这个词。

3. **无需外部输入的语义编码**  
   GPT 主要任务是基于已有文本直接生成内容，而不是先将输入编码成一个固定的语义向量再解码。它直接在“阅读”和“生成”中完成推理和理解，所以不需要 encoder 的帮助。

---

### 🔗 **那为什么像 BERT 只用 Encoder？**

BERT 的任务是 **理解**，比如分类、问答、文本相似度等。它使用的是 **双向注意力（bidirectional attention）**，可以同时看到上下文的所有信息（左边和右边的单词）。这对理解任务特别有用，但不适合生成任务，因为在生成时不能“偷看”未来的词。

---

### 🚀 **总结一下**

| 模型  | 结构         | 任务类型        | 特点                               |
|------|--------------|----------------|------------------------------------|
| BERT | 只用 Encoder  | 理解任务        | 双向注意力，适合分类、问答、摘要等 |
| GPT  | 只用 Decoder  | 生成任务        | 自回归生成，擅长对话、写作等       |
| T5   | Encoder-Decoder | 理解 + 生成 | 灵活处理各种 NLP 任务              |

---

如果以后你想了解 **encoder-decoder 混合架构** 的应用（比如 T5 或 BART），也可以问我！😄


---
title: "Reinforcement Learning Notes Week 2: Markov Decision Processes"
layout: post
date: 2025-05-13
categories: tech_coding
tags:
    - machine_learning
    - reinforcement_learning
---

## Lesson 1: Introduction to Markov Decision Processes

### Describe how the dynamics of an MDP are defined

一个 **Markov Decision Process (MDP)** 的动力学是由一个概率分布定义的。具体来说，给定一个当前状态 (*state*) $S$ 和一个在该状态下采取的动作 (*action*) $a$，动力学函数 $P$ 告诉我们转移到下一个状态 (*next state*) $S'$ 并获得奖励 (*reward*) $r$ 的联合概率。这个函数可以表示为 $p(S', r | S, a)$。**重要的是，未来状态和奖励只依赖于当前状态和采取的动作**，这被称为 **Markov property**。这意味着当前状态包含了足够的信息，不需要记住之前的状态来预测未来。



$p(S', r | S, a)$ 的完整含义是：

**当我处于状态 $S$，并且选择执行动作 $a$ 时，我将转移到下一个状态 $S'$ 并且获得奖励 $r$ 的可能性有多大？**

### Understand the graphical representation of a Markov Decision Process

文本中通过图示展示了 **Markov Decision Process (MDP)** 的概念。例如，在兔子寻找食物的例子中，图示展示了从一个状态（兔子所处的位置）出发，采取不同的动作（向左或向右移动）后，会转移到不同的下一个状态（例如，吃到胡萝卜、遇到老虎、吃到西兰花），并获得相应的奖励。对于回收机器人的例子，文本也描述了使用圆圈表示状态（高电量、低电量），并用箭头表示在不同状态下采取不同动作后的状态转移以及获得的奖励。这些图示通常会包含状态、动作以及状态之间的转移和相应的奖励。



### The Recycling Robot


### Explain how many diverse processes can be written in terms of the MDP framework

**Markov Decision Process (MDP)** 框架具有很高的灵活性，可以用来描述各种各样的顺序决策过程。文本中举了两个例子来说明这一点。第一个是回收机器人收集易拉罐的问题，这个例子展示了如何将一个实际问题抽象成由状态（电池电量）、动作（搜索、等待、充电）和奖励（收集到罐子、被救援等）组成的 **MDP**。第二个例子是控制机器人手臂完成抓取和放置任务，这个例子说明了 **MDP** 可以应用于复杂的控制问题，并且状态、动作和奖励的定义可以根据具体任务的需求进行调整，例如状态可以是关节角度和速度，动作可以是施加的电压，奖励可以是成功放置物体和消耗能量的负反馈。这些例子表明，无论是简单的环境交互还是复杂的控制任务，都可以用 **MDP** 的框架进行建模和分析。

### Understand Markov Decision Processes, or MDPs

**Markov Decision Process (MDP)** 提供了一个通用的框架，用于形式化顺序决策问题，即在需要做出连续决策的环境中，一个智能体（*agent*）如何选择动作以最大化其长期回报。与之前讨论的 k-Armed Bandit 问题不同，**MDP** 考虑了现实世界问题的两个**重要**方面：***不同的情况（状态）需要不同的响应，并且当前选择的动作会影响未来的奖励。*** 在 **MDP** 中，智能体在离散的时间步与环境进行交互。在每个时间步，智能体从环境中接收到一个状态 (*state*) $S_t$，然后根据这个状态选择一个动作 (*action*) $A_t$。执行动作后，环境会转移到一个新的状态 $S_{t+1}$，并给智能体一个标量奖励 (*reward*) $R_{t+1}$。这个交互过程会产生一个由状态、动作和奖励组成的轨迹。**MDP** 的目标是找到一个策略，即在每个状态下选择哪个动作，以最大化智能体在整个过程中的累积奖励。

## Lesson 2: Goal of Reinforcement Learning

### Describe how rewards relate to the goal of an agent


奖励（rewards）与智能体（agent）的目标相关联的方式是：智能体的目标是最大化未来的奖励。更具体地说，智能体试图最大化在时间步 t 之后获得的所有奖励的总和，这被称为“return” (G)。由于环境的动态可能是随机的，智能体实际上最大化的是“expected return”。仅仅最大化即时奖励可能不是最优的，因为一个在短期内看起来不错的动作可能会导致未来获得较低的奖励。因此，智能体需要考虑长期回报。


### Understand episodes and identify episodic tasks

"episodes" 是指智能体与环境的交互自然地分解成的块。每个 "episode" 独立于前一个 "episode" 的结束而开始。当一个 "episode" 结束时，智能体会重置到一个 "start state"。每个 "episode" 都有一个最终状态，称为 "terminal state"。将任务分解为这些 "episodes" 的任务被称为 "episodic tasks"。例如，一局国际象棋游戏就是一个 "episode"，它总是以将死、平局或认输结束，并且每局游戏都从相同的 "start state" 开始。

## Lesson 3: Continuing Tasks
好的，这是用 Markdown 格式画出的表格：

| Episodic Tasks                                  | Continuing Tasks                                    |
| :---------------------------------------------- | :-------------------------------------------------- |
| • Interaction breaks naturally into episodes     | • Interaction goes on continually                   |
| • Each episode ends in a terminal state        | • No terminal state                                  |
| • Episodes are independent                      |                                                     |
| • $G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T$ |                                                     |
### Formulate returns for continuing tasks using discounting

对于 **continuing tasks**，回报在时间步 *t* ($G_t$) 的制定需要使用 **discounting** 来确保其是有限的，因为交互是无限进行的。这通过引入一个折扣因子 **Gamma** ($\gamma$) 来实现，其中 $0 \leq \gamma < 1$。回报然后被计算为未来奖励的总和，每个奖励都通过 $\gamma$ 的幂进行折扣，幂的值是其未来步数：

$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}  = R_{max} \times \frac{1}{1-\gamma}$

这种 **discounting** 确保了即时奖励比未来很久的奖励对总和的贡献更大。

```
When Gamma equals zero the return is just the reward at the next time step. So the agent is shortsighted and only cares about immediate expected reward. On the other hand, when Gamma approaches one, the immediate and future rewards are weighted nearly equally in the return. The agent in this case is more farsighted.
```

### Describe how returns at successive time steps are related to each other

在 **continuing task** 中，连续时间步的回报之间存在递归关系。在时间 *t* 的回报 ($G_t$) 可以用下一个时间步的回报 ($G_{t+1}$) 表示如下：

$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$
$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots)$
$G_t = R_{t+1} + \gamma G_{t+1}$

这个递归关系表明，在任何给定时间步的回报是即时奖励加上从下一个时间步开始的折扣回报。

### Understand when to formalize a task as episodic or continuing

当一个任务可以自然地分解为独立的 **episodes**，并且每个 **episode** 都有一个明确的在 **terminal state** 结束时，应该将其形式化为 **episodic**。到达 **terminal state** 后，下一个 **episode** 的开始与上一个 **episode** 如何结束无关。例如，玩一个视频游戏，当满足特定条件（如碰到敌人）时游戏结束，并且下一局游戏从预定义的初始状态开始。

当智能体与环境的交互无限期地进行，没有任何自然的结束或 **terminal states** 时，应该将任务形式化为 **continuing**。例如，一个调节温度的智能恒温器，它持续地与环境交互，以及一个不断接收和处理作业的作业调度系统。在 **continuing tasks** 中，这个过程永远不会停止。


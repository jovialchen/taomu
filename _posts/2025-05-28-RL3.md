---
title: "Reinforcement Learning Notes Week 2: Value Functions and Bellman Equations"
layout: post
date: 2025-05-28
categories: tech_coding
tags:
    - reinforcement_learning
---



## Lesson 1: Policies and Value Functions

### Recognize that a policy is a distribution over actions for each possible state

一个策略（policy）可以被理解为在每个可能的状态（state）下，采取不同动作（action）的概率分布。


 

### Describe the similarities and differences between stochastic and deterministic policies

- **Stochastic Policy:** 在一个给定的状态下，采取不同动作的概率可以大于零，即存在随机性。
-  **Deterministic Policy:** 在一个给定的状态下，只会以概率 1 采取某一个特定的动作，没有随机性。
    * **相似性:** 它们都是在特定状态下指导 agent 行为的方式。
    * **差异性:** Stochastic policy 允许探索不同的动作，而 deterministic policy 则只利用已知的最优动作。

 

### Identify the characteristics of a well-defined policy

 It's important that policies depend only on the **current state**, not on other things like time or previous states. The state defines all the information used to select the current action.
It is better to think of this as a requirement on the state, not a limitation on the agent.  

### Generate examples of valid policies for a given MDP
在一个网格世界 MDP 中，agent 在每个状态下采取上下左右四个动作的概率是相同的。这是一个有效的策略，因为它为每个状态定义了明确的动作选择方式。
 

### Describe the roles of state-value and action-value functions in reinforcement learning
好的，请看以下三个问题的答案：

 **在强化学习中，状态价值函数（state value function）和动作价值函数（action value function）的角色是什么？**

    状态价值函数（state value function）粗略地说，是从一个特定状态开始，一个智能体（agent）可以期望获得的未来奖励（future reward）。更精确地说，它是从给定状态开始的期望回报（expected return）。它衡量了在遵循特定策略 $\Pi$ 的情况下，从某个状态开始的预期总奖励。
$$v_\pi(s) \doteq \mathbb{E}_\pi [G_t | S_t = s]$$
Recall that : 
$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

    动作价值函数（action value function）描述了当智能体首先选择一个特定的动作（action）时会发生什么。更正式地说，一个状态的动作价值是在智能体选择动作 $A$ 之后，并遵循策略 $\Pi$ 所能获得的期望回报（expected return）。

$$q_\pi(s, a) \doteq \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$$

    总的来说，价值函数在强化学习中至关重要，它们允许智能体查询当前情况的质量，而不是等待观察长期的结果。



### Describe the relationship between value functions and policies
 **价值函数和策略（policies）之间有什么关系？**

    一个价值函数是相对于一个给定的策略（policy）定义的。策略 $\Pi$ 的下标表示价值函数依赖于智能体根据策略 $\Pi$ 选择动作。同样，期望值上的下标 $\Pi$ 表示期望是根据策略 $\Pi$ 计算的。价值函数能够帮助我们判断不同策略的质量。
### Create examples of valid value functions for a given MDP
 
 **为给定的马尔可夫决策过程（MDP）创建价值函数的例子。**

    考虑一个简单的持续性马尔可夫决策过程（continuing MDP），其状态由网格上的位置定义，动作包括上、下、左、右移动。智能体不能移出网格，撞到边界会产生 -1 的奖励。大多数其他动作不产生奖励。然而，有两个特殊状态 A 和 B。在状态 A 中，每个动作都会产生 +10 的奖励，并转移到状态 A'；在状态 B 中，每个动作都会产生 +5 的奖励，并转移到状态 B'。

    假设我们采用一个均匀随机策略（uniform random policy），并且折扣因子 $\gamma = 0.9$。在这种情况下，每个状态都有一个对应的状态价值。例如，靠近底部边界的状态具有较低的负值，因为智能体在到达奖励状态 A 和 B 之前很可能撞到墙壁。状态 A 具有最高的价值，但即使从状态 A 执行每个动作都会产生 +10 的奖励，其价值也小于 10，因为从 A 的每次转移都会使智能体靠近下方的墙壁，而随机策略很可能会导致撞墙并获得负奖励。另一方面，状态 B 的价值略高于 5，因为从 B 的转移会将智能体移动到中间位置，在那里智能体不太可能撞墙，并且靠近高价值状态 A 和 B。

    在这个例子中，状态价值函数总结了在给定策略下，从每个状态开始的预期未来奖励总和。 
![[[week3 history of RL]]]({1B4450AA-0BA0-4091-8F84-AC92A2026BBE}.png)

## Lesson 2: Bellman Equations

### Derive the Bellman equation for state-value functions

状态价值函数 $v_\pi(s)$ 定义为从状态 $s$ 开始的期望回报（expected return）。回报 $G_t$ 又被定义为未来奖励的折扣和（discounted sum of future rewards），并且可以递归地写成即时奖励 $R_{t+1}$ 加上折扣后的 $G_{t+1}$。

推导过程首先将期望回报展开为智能体可能采取的动作选择的和，然后进一步展开为在状态 $s$ 和动作 $a$ 条件下可能出现的奖励和下一个状态 $s'$ 的和。由于政策（policy）和转移概率（dynamics function）不依赖于时间，我们可以将表达式中关于未来期望回报的部分替换为下一个状态 $s'$ 的价值函数 $v_\pi(s')$。最终，我们得到状态价值函数的Bellman方程，它将一个状态的价值与它可能的后续状态的价值建立了关系。

Here's the LaTeX from the image, broken down by line:
$$
\begin{align*}
\textcolor{red}{v_{\pi}(s)} &\doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]\\
&= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right]\\
&= \sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right]\right]\\
&= \sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \textcolor{red}{v_{\pi}\left(s^{\prime}\right)}\right]
\end{align*}
$$
Recall that
$$G_{t}=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}$$

### Derive the Bellman equation for action-value functions
 
动作价值函数 $q_\pi(s, a)$ 的Bellman方程是一个关于状态-动作对 $(s, a)$ 的价值的递归方程，它基于其可能的后续状态-动作对。与状态价值函数的Bellman方程不同，动作价值函数的推导不以策略选择动作开始，因为动作 $a$ 已经作为状态-动作对的一部分被固定了。

推导从直接考虑动态函数 $p$ 来选择即时奖励 $R_{t+1}$ 和下一个状态 $S'$ 开始。我们得到了一个加权和，包含即时奖励加上给定特定下一个状态 $s'$ 的期望未来回报。为了将其表示为关于下一个状态-动作对的递归方程，我们需要将从下一个状态 $s'$ 开始的期望回报表示为智能体在 $s'$ 处可能采取的动作选择的和。具体来说，我们可以将期望条件化为下一个状态 $S'$ 和下一个动作 $A'$，然后对所有可能的动作 $A'$ 求和，每个项由在状态 $S'$ 下根据策略 $\Pi$ 选择 $A'$ 的概率加权。此时，这个期望回报就等同于下一个状态-动作对 $(S', A')$ 的动作价值函数 $q_\pi(S', A')$。替换后，我们就得到了动作价值函数的Bellman方程，它将一个状态-动作对的价值与可能的下一个状态-动作对的价值联系起来。 
$$\begin{align*}
\textcolor{red}{q_{\pi}(s, a)} &\doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]\\
&= \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right]\right]\\
&= \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \sum_{a^{\prime}} \pi\left(a^{\prime} \mid s^{\prime}\right) \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s^{\prime}, A_{t+1}=a^{\prime}\right]\right]\\
&= \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \sum_{a^{\prime}} \pi\left(a^{\prime} \mid s^{\prime}\right) \textcolor{red}{q_{\pi}\left(s^{\prime}, a^{\prime}\right)}\right]
\end{align*}$$

### Understand how Bellman equations relate current and future values

Bellman方程（Bellman equations）将一个状态的价值（value）与其可能的后续状态（successor states）的价值联系起来。它形式化了当前状态的价值与未来状态的价值之间的这种联系，而无需等待观察所有未来的奖励。通过Bellman方程，我们可以将未来无限时间步长的期望回报，递归地表示为即时奖励（immediate reward）加上折扣后的下一状态的价值。 

### Use the Bellman equations to compute value functions

#### 小例子：四格网格

为了说明贝尔曼方程的强大之处，我们来看一个由A、B、C、D四个状态组成的简单网格示例。

* **动作空间**：上下左右移动。如果移动出网格，则会停留在原地。
* **示例路径**：从C出发，向上到A。从A向左会撞墙停留在A。然后向右到B。从B向下会到达D。
* **奖励设置**：除了到达B状态时获得+5的奖励外，其他所有地方的奖励都是0。即使从B状态开始或在B状态撞墙，也会获得+5的奖励。
* **策略**：我们采用**均匀随机策略**，即每个方向移动的概率都是25%。
* **折扣因子**：$\gamma = 0.7$。

那么，在这种策略下，我们如何计算A、B、C、D每个状态的价值呢？


#### 价值函数与贝尔曼方程

回想一下，**价值函数**定义为在特定策略下预期的回报。这涉及到代理可能选择的无限多种动作序列所获得的回报的平均值。幸运的是，用于状态价值函数的贝尔曼方程提供了一个优雅的解决方案。

利用贝尔曼方程，我们可以写出A状态价值的表达式，它是四种可能动作及其导致后继状态的总和。在这个例子中，我们可以进一步简化表达式，因为对于每个动作，只有一个可能的关联下一个状态和奖励。这意味着对$s'$和$r$的求和会简化为一个单一的值。请注意，这里的$s'$和$r$仍然依赖于所选的动作和当前状态$s$，但为了简化符号，我们没有明确表示。
$$V_{\pi}(s) = \sum_a \pi(a|s) \sum_r \sum_{s'} p(s',r|s,a)[r + \gamma V_{\pi}(s')]$$
 **奖励和下一个状态的唯一性：** 视频中提到：“We can simplify the expression further in this case, because for each action there's only one possible associated next state and reward. That's the sum over s prime and r reduces to a single value.”
    这意味着：
    * 对于在状态 A 下采取的**每一个特定动作** $a$，都会确定性地导致**一个唯一的下一个状态** $s'$ 和**一个唯一的即时奖励** $r$。
    * 因此，内部的 $\sum_r \sum_{s'} p(s',r|s,a)$ 部分，因为 $p(s',r|s,a)$ 对于特定的 $s, a$ 只有一个 $s'$ 和 $r$ 的组合其概率为 1，其他为 0，所以这个求和**简化掉了**。它不再是一个复杂的求和，而是直接取那个唯一的 $r$ 和 $s'$。

$$V_{\pi}(A) = \sum_a \pi(a|A)(r + 0.7V_{\pi}(s'))$$
* **从A向右**：到达B状态，获得+5奖励。在随机策略下，这种情况发生的概率是1/4。
* **从A向下**：到达C状态，没有即时奖励。这种情况也发生在1/4的时间。
* **从A向上或向左**：都会回到A状态，没有奖励。由于这两种动作各占1/4的概率，且结果相同，我们可以将它们合并为一个1/2的项。
$$V_{\pi}(A) = \frac{1}{4}(5 + 0.7V_{\pi}(B)) + \frac{1}{4}0.7V_{\pi}(C) + \frac{1}{2}0.7V_{\pi}(A)$$
最终，我们得到了A状态价值的表达式。我们可以为其他状态B、C、D写出类似的方程。这样我们就得到了一个包含四个变量的四元方程组。你可以选择手动求解，也可以使用自动方程求解器。

以下是图片中包含的 LaTeX 脚本：

$$
V_{\pi}(B) = \frac{1}{2}(5 + 0.7V_{\pi}(B)) + \frac{1}{4}0.7V_{\pi}(A) + \frac{1}{4}0.7V_{\pi}(D)
$$

$$
V_{\pi}(C) = \frac{1}{4}0.7V_{\pi}(A) + \frac{1}{4}0.7V_{\pi}(D) + \frac{1}{2}0.7V_{\pi}(C)
$$

$$
V_{\pi}(D) = \frac{1}{4}(5 + 0.7V_{\pi}(B)) + \frac{1}{4}0.7V_{\pi}(C) + \frac{1}{2}0.7V_{\pi}(D)
$$
#### 贝尔曼方程的强大之处


贝尔曼方程将一个难以处理的、关于可能未来的无限求和，简化为一个简单的**线性代数问题**。
也许对于这个小问题，你可以想出其他方法来计算每个状态的价值。然而，贝尔曼方程为马尔可夫决策过程（MDP）提供了一个强大的通用关系。在这种情况下，我们利用贝尔曼方程直接写出了状态价值的方程组，然后求解该系统以找到价值。这种方法对于中等规模的MDP可能可行。

#### 贝尔曼方程的局限性
然而，在更复杂的问题中，这并不总是实际的。例如，考虑国际象棋游戏。我们可能甚至无法列出所有可能的**状态**（大约有$10^{45}$个）。构建和求解由此产生的贝尔曼方程系统将是另一个层面的挑战。



## Lesson 3: Optimality (Optimal Policies & Value Functions)



### **1. Define an optimal policy (定义最优策略)**

根据文本，最优策略的定义如下：

An optimal policy is a policy which is as good as or better than all the other policies. That is, an optimal policy will have the highest possible value in every state. We'll use the notation $\Pi^*$ to denote any optimal policy.

简而言之，**最优策略是在所有状态下都具有最高可能价值的策略。**


### Understand how a policy can be at least as good as every other policy in every state


* **策略比较的定义**：我们说策略 $\Pi_1$ 优于或等同于策略 $\Pi_2$（表示为 $\Pi_1 \ge \Pi_2$），当且仅当在**每一个状态**下，$\Pi_1$ 的价值 ($V^{\Pi_1}(s)$) 都大于或等于 $\Pi_2$ 的价值 ($V^{\Pi_2}(s)$)。
* **构建更优策略的可能性**：文本通过一个非正式的论证来解释：
    * 假设存在两个策略 $\Pi_1$ 和 $\Pi_2$，其中 $\Pi_1$ 在某些状态下表现好，而 $\Pi_2$ 在另一些状态下表现好。
    * 我们可以构建一个新的策略 $\Pi_3$，它在每个状态下都选择 $\Pi_1$ 和 $\Pi_2$ 中具有更高价值的动作。
    * 这样做，$\Pi_3$ 在每个状态下的价值必然大于或等于 $\Pi_1$ 和 $\Pi_2$ 的价值。
    * 这意味着，我们永远不会遇到“在一个状态中表现良好就必须牺牲另一个状态的价值”的情况。
* **严格证明的存在**：虽然文本提供了非正式论证，但它也指出，实际上存在严格的证明表明，至少存在一个最优的确定性策略。


### Identify an optimal policy for given MDPs

文本通过一个具体的“两选择MDP”示例来演示如何识别最优策略，并强调了折扣因子 $\Gamma$ 的重要性：

* **计算每个策略的价值函数**：对于一个给定的MDP，识别最优策略的核心方法是计算每个可能策略的价值函数。价值最高的策略就是最优策略。
* **折扣因子 ($\Gamma$) 的影响**：
    * **$\Gamma = 0$ (只考虑即时奖励)**：在这种情况下，价值仅由即时奖励决定。例如，如果策略 $\Pi_1$ 带来即时奖励 +1，而策略 $\Pi_2$ 带来延迟奖励 +2，那么当 $\Gamma = 0$ 时，$\Pi_1$ 更优（价值为 +1，而 $\Pi_2$ 的价值为 0，因为延迟奖励被忽略）。For $\gamma = 0$:

 $$v_{\pi_1}(X) = 1$$
 $$v_{\pi_2}(X) = 0$$


  * **$\Gamma > 0$ (考虑长期奖励)**：当 $\Gamma$ 接近 1 时，未来的奖励会被更多地考虑进来。在这种情况下，策略 $\Pi_2$ 可能因为其更大的延迟奖励（即使有延迟）而变得更优。文本中的例子展示了当 $\Gamma = 0.9$ 时，$\Pi_2$ 的价值 (9.5) 远高于 $\Pi_1$ 的价值 (5.3)，因此 $\Pi_2$ 成为最优策略。For $\gamma = 0.9$:

$$v_{\pi_1}(X) = \sum_{k=0}^{\infty} (0.9)^{2k} = \frac{1}{1 - 0.9^2} \approx 5.3$$
$$v_{\pi_2}(X) = \sum_{k=0}^{\infty} (0.9)^{2k+1} * 2 = \frac{0.9}{1 - 0.9^2} * 2 \approx 9.5$$
* **暴力搜索的局限性**：虽然通过计算所有策略的价值来识别最优策略对于简单MDP可行，但对于复杂MDP，可能的策略数量会呈指数级增长，使得这种暴力搜索方法变得不可行。

 

 

 

 

 

### Derive the Bellman optimality equation for state-value functions

推导 **state-value function** 的 **Bellman optimality equation** 的核心在于利用最优策略的特性。我们从**Bellman equation** for $v_\pi(s)$ 开始，这个方程对任意策略 $\pi$ 都成立。

首先，我们知道对于**最优策略** $\Pi^*$，其对应的状态价值函数是 $v^*(s)$。因此，我们可以将 $v_\pi(s)$ 替换为 $v^*(s)$，并将策略 $\pi$ 替换为 $\Pi^*$。

关键的一步是，**最优策略** $\Pi^*$ 具有一个特殊的性质：在任何状态 $s$ 下，它总是选择能带来最高价值的动作 $a$。这意味着我们可以用对所有可能动作 $a$ 的最大化操作 (**max over actions**) 来代替原方程中对策略 $\Pi^*$ 下动作选择的求和。

具体来说，原先的 **Bellman equation** 包含了对策略 $\Pi^*$ 在状态 $s$ 下选择动作 $a$ 的概率 $P(a|s, \Pi^*)$ 的求和。由于最优策略会确定性地选择最佳动作（即该动作的概率为1，其他动作概率为0），我们可以直接取所有可能动作中能够产生最大未来回报的那个动作。

因此，**Bellman optimality equation for $v^*(s)$** 可以表示为：

$v^*(s) = \max_a \sum_{s', r} P(s', r | s, a) [r + \gamma v^*(s')]$

这个方程的意义在于，一个状态的最优价值等于在该状态下选择能带来最高未来回报的动作所能获得的价值。


### Derive the Bellman optimality equation for action-value functions
**action-value function** $q^*(s,a)$ 的 **Bellman optimality equation** 的推导思路与状态价值函数类似。

我们从 **Bellman equation** for $q_\pi(s,a)$ 开始。这个方程表示在状态 $s$ 执行动作 $a$ 之后，在策略 $\pi$ 下的预期回报。

对于 **最优动作价值函数** $q^*(s,a)$，它对应的是最优策略 $\Pi^*$ 下在状态 $s$ 执行动作 $a$ 所能获得的最大预期回报。在执行动作 $a$ 之后，系统将进入下一个状态 $s'$。在 $s'$ 状态下，为了实现最优，我们应该选择能最大化未来回报的动作。

因此，**Bellman optimality equation for $q^*(s,a)$** 可以表示为：

$q^*(s,a) = \sum_{s', r} P(s', r | s, a) [r + \gamma \max_{a'} q^*(s', a')]$

这个方程表明，一个状态-动作对的最优价值，等于执行该动作后立即获得的奖励，加上折现后的下一个状态所有可能动作中最大化的最优动作价值。


### Understand how the Bellman optimality equations relate to the previously introduced Bellman equations
**Bellman optimality equations** 与普通的 **Bellman equations** 紧密相关，但存在关键的区别：

* **关系：**
    * **Bellman optimality equations** 是 **Bellman equations** 在**最优策略** (optimal policy) 情况下的特殊形式。它们仍然遵循动态规划的原则，即将一个复杂问题分解为子问题，并且通过递归关系来求解。
    * 它们都描述了价值函数与其后续状态或状态-动作对的价值之间的关系。

* **区别：**
    * **是否包含策略：** 普通的 **Bellman equations** 明确依赖于一个给定的策略 $\pi$，方程中通常包含对策略下动作选择概率的求和。而 **Bellman optimality equations** 的目标是找到**最优价值函数**，它们通过引入**最大化操作 (max over actions)** 来消除对具体策略 $\pi$ 的依赖，从而直接指向最优解。
    * **线性与非线性：** 这是最重要的区别。
        * 普通的 **Bellman equations** （尤其是对于给定策略的 $v_\pi(s)$）通常形成一个线性方程组，可以使用标准的线性代数方法（如矩阵求逆）来直接求解。
        * 然而，**Bellman optimality equations** 由于引入了 **max** 运算符，使其变为**非线性方程组**。这意味着我们不能简单地使用线性代数方法来直接求解它们。这使得求解最优价值函数成为强化学习中一个更具挑战性的问题，通常需要迭代方法（如 **Value Iteration** 或 **Policy Iteration**）来近似求解。
    * **求解目标：** 普通的 **Bellman equations** 用于评估一个已知策略的价值。而 **Bellman optimality equations** 的根本目标是找到**最优价值函数**，进而从最优价值函数中推导出**最优策略**。我们通常在不知道**最优策略** $\Pi^*$ 的情况下尝试求解 **Bellman optimality equations**。




### Understand the connection between the optimal value function and optimal policies

* **目标一致性**：尽管最终目标是找到最优策略 ($\pi^*$)，但实际上，找到最优价值函数 ($v^*$) 和找到最优策略是**几乎相同的**问题。这意味着一旦你拥有了其中一个，就可以相对容易地推导出另一个。
* **$v^*$ 导出 $\pi^*$ 的过程**：
    * 如何从已知的 $v^*$ 来确定 $\pi^*$。对于每个状态，通过“一步展望” (one-step look-ahead) 来评估每个可能动作的价值。这个价值是即时奖励加上下一个状态的折现最优价值 ($\gamma v^*(s')$)。
    * **贝尔曼最优方程 (Bellman Optimality Equation)**：将这个过程与贝尔曼最优方程联系起来，指出 $v^*$ 是方程右侧所有动作的最大值，而 $\pi^*$ 则是实现这个最大值的动作 (argmax)。
    * **确定性与随机性策略**：如果多个动作都能达到最大值，那么任何选择这些最大化动作的策略都是最优策略，这可以是确定性的（选择其中一个）或随机性的（以一定概率在它们之间选择）。
$$v_*(s) = \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v_*(s')]$$
$$\pi_*(s) = \text{argmax}_a \sum_{s', r} p(s', r | s, a)[r + \gamma v_*(s')]$$
* **$q^*$ 简化过程**：如果拥有 **最优动作价值函数** ($q^*$)，找到最优策略会更简单，因为它直接缓存了每个状态-动作对的“一步展望”结果，只需选择使 $q^*(s, a)$ 最大的动作即可。







### 验证给定 MDP 的最优价值函数 (Verify the optimal value function for given MDPs)
* **通过贝尔曼最优方程进行验证**：文字通过具体例子（网格世界）展示了如何验证一个已知的 $v^*$ 是否确实是最优价值函数。
    * 对于某个状态，通过计算其最优动作所对应的一步展望值（即贝尔曼最优方程的右侧），如果这个值等于该状态的 $v^*$，则表明 $v^*$ 在该状态下满足贝尔曼最优方程。
    * 通过对网格世界中多个状态进行这种验证，文字说明了 $v^*$ 如何在所有状态下遵守贝尔曼最优方程。
* **最优策略的推导作为验证的副产品**：在推导最优策略的过程中，实际上也在验证 $v^*$。当根据 $v^*$ 计算每个动作的期望收益并选择最优动作时，如果这个最优动作的期望收益与 $v^*$ 本身相等，就验证了 $v^*$ 的正确性。这表明 $v^*$ 不仅能导出最优策略，而且其自身也满足了最优性条件。
 

这个题目的计算就是: 某个action, action的reward+gamma*下一个state的state value
完全带入policy的那个公式即可.

 



